# LLM in a FLASH: 利用闪存高效运行大语言模型

> Author by：漆翔宇

## 引言

随着大语言模型（LLMs）在自然语言处理领域取得突破性进展，它们在各种任务中展现出卓越的性能。然而，这些模型庞大的计算和内存需求也带来了严峻的挑战，尤其是在DRAM（动态随机存取存储器）容量有限的设备上。传统的做法是将整个模型参数加载到DRAM中进行推理，但这对于参数量动辄数百亿甚至上千亿的大模型来说，迅速超出了许多设备（特别是消费级硬件或边缘设备）的DRAM承载能力。这不仅限制了LLM的广泛部署，也导致了高昂的运行成本。

为了解决这一核心瓶颈，Apple提出了一种创新的方法，**通过将模型参数存储在闪存（flash memory）中，并根据需要按需（on demand）加载到DRAM中，从而高效地运行超出DRAM容量限制的大语言模型**。这种方法在保持高性能的同时，极大地拓展了LLM的部署边界。

## 1. 背景与挑战

### 大模型与DRAM瓶颈

大语言模型的核心是其海量的参数，这些参数在推理过程中需要被快速访问。DRAM作为计算机系统中最快的内存层次之一，通常用于存储这些参数。然而，随着模型规模的爆炸式增长，即使是高端GPU通常配备的几十GB DRAM也显得捉襟见肘。例如，一个70亿参数的模型（FP16精度）需要约14GB的显存，而一个1750亿参数的模型则需要约350GB显存。对于消费级显卡（如12GB或24GB）或普通CPU系统而言，直接加载这类大型模型是不可行的。

DRAM的另一个限制是其高昂的成本，这使得配备超大容量DRAM的设备变得非常昂贵。因此，如何在有限的DRAM容量下高效运行甚至更大的模型，成为LLM领域亟待解决的关键问题。

### 闪存的潜力与挑战

闪存（如SSD中使用的NAND Flash）提供了一种替代方案。相比DRAM，闪存具有以下显著优势：
*   **容量更大：** 闪存的存储密度远高于DRAM，因此可以提供TB级别甚至更大容量的存储。
*   **成本更低：** 单位存储容量的闪存成本远低于DRAM。

然而，闪存并非没有挑战。尽管现代闪存提供了高带宽，但其访问特性与DRAM截然不同，这在设计算法时必须考虑：
*   **随机读写延迟较高：** 相比DRAM的纳秒级访问，闪存的随机读写延迟要高得多。小块、随机的读写操作效率低下。
*   **顺序读写效率高：** 闪存最擅长的是大块、连续的顺序读写。例如，在Apple MacBook M1 Max上，1TB闪存的顺序读取速度可超过6 GiB/s，但小块随机读取则因操作系统、驱动、中断处理和闪存控制器等开销而无法达到这一性能。

  因此，如何在利用闪存大容量低成本优势的同时，规避其随机访问的劣势，是高效利用闪存运行LLM的关键。

## 2. LLM in a FLASH 核心思想

“LLM in a FLASH”方法的核心思想围绕着**将模型参数存储在闪存中，并仅在计算需要时才按需加载到DRAM**。为了实现这一目标，它构建了一个深入理解闪存硬件特性的推理成本模型，并在此基础上，专注于以下两个关键优化方向：

1.  **减少从闪存传输的数据量：** 避免加载不必要的参数，从而最小化闪存到DRAM的数据移动。
2.  **以更大、更连续的块读取数据：** 优化数据访问模式，以最大化利用闪存的顺序读写带宽。

通过这些优化，该方法旨在弥合DRAM和闪存之间的性能鸿沟，使LLM在有限DRAM设备上也能高效运行。


## 3. 关键技术详解

为了实现上述优化目标，论文引入了两个主要的技术：**Windowing（窗口化）** 和 **Row-Column Bundling（行列捆绑）**。

### 1. Windowing (窗口化)

*   **目的：** 策略性地减少数据传输量，通过重用先前激活的神经元来避免重复加载。
*   **工作原理：** 大语言模型在推理过程中，并不是所有的参数都会在每个时间步或每个层中被激活。Windowing技术利用这种**激活稀疏性**，将模型参数（尤其是那些与中间激活相关的参数）分解成更小的“窗口”或块。当模型进行推理时，只有当前计算所需的小部分参数会被加载到DRAM。此外，系统会智能地管理这些加载到DRAM的“窗口”，尽可能地重用已加载的参数，而不是每次都从闪存重新加载。这类似于CPU缓存的工作原理，通过局部性原理减少了对慢速存储的访问次数。

  
### 2. Row-Column Bundling (行列捆绑)

*   **目的：** 针对闪存的顺序数据访问优势，增加从闪存读取的数据块大小和连续性。
*   **工作原理：** 如前所述，闪存对大块顺序读取的性能远优于小块随机读取。Row-Column Bundling技术旨在优化模型参数的存储布局，使其能够以闪存友好的方式进行访问。具体来说，它将通常以矩阵形式存储的模型权重，进行重新组织或“捆绑”，将相关的行和列数据打包成更大的、连续的块。当需要读取这些权重时，系统可以一次性从闪存中读取一个大块，而不是分散地读取许多小块，从而最大化闪存的传输带宽，显著提高数据加载效率。

### 辅助优化：稀疏性感知与上下文自适应加载

除了上述两项主要技术，“LLM in a FLASH”还集成了**稀疏性感知（Sparsity Awareness）** 和 **上下文自适应加载（Context-Adaptive Loading）**。稀疏性感知意味着系统能够识别模型中不活跃或重要性较低的参数，并优先处理或跳过它们的加载。上下文自适应加载则根据当前的输入和计算上下文，动态地调整参数的加载策略，进一步精细化按需加载的粒度，确保只有最相关的参数才被加载到DRAM。这些辅助优化共同构成了“LLM in a FLASH”在硬件感知框架下实现高效推理的关键。


## 4. 实验结果与性能

“LLM in a FLASH”在多个实验中展现了令人印象深刻的性能提升。

*   **模型规模支持：** 该方法成功地使得在可用DRAM容量**两倍大**的模型上进行推理成为可能。这意味着，如果一个设备只有16GB DRAM，理论上可以运行高达32GB的模型。
*   **推理速度提升：** 相较于CPU上的简单加载方法，推理速度提升了 **4-5倍**。而在GPU上，推理速度更是实现了惊人的 **20-25倍** 提升。这充分证明了其在实际应用中的巨大潜力。

这些结果表明，“LLM in a FLASH”为在内存受限设备上部署大型语言模型开辟了新的途径。

## 5. 总结与思考

“LLM in a FLASH”通过将模型参数存储在闪存并按需加载到DRAM的策略，成功地应对了大语言模型在有限DRAM设备上运行的挑战。其核心创新点包括：

*   **硬件感知的设计：** 深入分析闪存的读写特性，并据此设计优化策略。
*   **Windowing（窗口化）：** 利用模型激活的稀疏性，通过重用激活神经元来减少数据传输量。
*   **Row-Column Bundling（行列捆绑）：** 优化数据存储布局，使闪存能够进行大块、连续的顺序读取，从而最大化传输效率。

这项技术的影响深远。它极大地降低了运行大型语言模型所需的硬件门槛，使得更广泛的设备（包括消费级笔记本电脑、智能手机甚至嵌入式设备）能够本地运行曾经只能在高端服务器上部署的LLM。这不仅能降低推理成本，还能提高数据隐私和安全性，为大模型应用的普及化奠定了基础。

## 参考文献

1.  Alizadeh, K., Mirzadeh, I., Belenko, D., Khatamifard, K., Cho, M., Del Mundo, C. C., ... & Farajtabar, M. (2023). **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**. *arXiv preprint arXiv:2312.11514*.

