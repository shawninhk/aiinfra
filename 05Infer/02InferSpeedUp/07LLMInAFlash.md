# LLM in a FLASH: 利用闪存高效运行大语言模型

> Author by：漆翔宇

## 引言

随着大语言模型（LLMs）在自然语言处理领域取得突破性进展，它们在各种任务中展现出卓越的性能。然而，这些模型庞大的计算和内存需求也带来了严峻的挑战，尤其是在DRAM（动态随机存取存储器）容量有限的设备上。传统的做法是将整个模型参数加载到DRAM中进行推理，但这对于参数量动辄数百亿甚至上千亿的大模型来说，迅速超出了许多设备（特别是消费级硬件或边缘设备）的DRAM承载能力。这不仅限制了LLM的广泛部署，也导致了高昂的运行成本。

为了解决这一核心瓶颈，Apple提出了一种创新的方法，**通过将模型参数存储在闪存（flash memory）中，并根据需要按需（on demand）加载到DRAM中，从而高效地运行超出DRAM容量限制的大语言模型**。这种方法在保持高性能的同时，极大地拓展了LLM的部署边界。

## 1. 背景与挑战

### 大模型与DRAM瓶颈

大语言模型的核心是其海量的参数，这些参数在推理过程中需要被快速访问。DRAM作为计算机系统中最快的内存层次之一，通常用于存储这些参数。然而，随着模型规模的爆炸式增长，即使是高端GPU通常配备的几十GB DRAM也显得捉襟见肘。例如，一个70亿参数的模型（FP16精度）需要约14GB的显存，而一个1750亿参数的模型则需要约350GB显存。对于消费级显卡（如12GB或24GB）或普通CPU系统而言，直接加载这类大型模型是不可行的。

DRAM的另一个限制是其高昂的成本，这使得配备超大容量DRAM的设备变得非常昂贵。因此，如何在有限的DRAM容量下高效运行甚至更大的模型，成为LLM领域亟待解决的关键问题。

### 闪存的潜力与挑战

闪存（如SSD中使用的NAND Flash）提供了一种替代方案。相比DRAM，闪存具有以下显著优势：
*   **容量更大：** 闪存的存储密度远高于DRAM，因此可以提供TB级别甚至更大容量的存储。
*   **成本更低：** 单位存储容量的闪存成本远低于DRAM。
