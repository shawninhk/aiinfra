{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964e933a",
   "metadata": {},
   "source": [
    "<!--Copyright Â© ZOMI é€‚ç”¨äº[License](https://github.com/Infrasys-AI/AIInfra)ç‰ˆæƒè®¸å¯-->\n",
    "\n",
    "# CODE 03: Pipeline å¹¶è¡Œå®è·µ\n",
    "\n",
    "æœ¬å®éªŒæ—¨åœ¨æ·±å…¥ç†è§£ Pipeline å¹¶è¡ŒåŸç†ã€‚å…ˆå®ç° Gpipe æµæ°´çº¿å¹¶åˆ†æç©ºæ³¡ç‡ç°è±¡ï¼Œåè¿›é˜¶å®ç° 1F1B å’Œ Interleaved 1F1B è°ƒåº¦ç­–ç•¥ï¼Œä¼˜åŒ–ç©ºæ³¡ç‡ç°è±¡ï¼Œå¹¶å®è·µæ··åˆå¹¶è¡Œç­–ç•¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a924ba",
   "metadata": {},
   "source": [
    "## 1. Pipeline å¹¶è¡ŒåŸºç¡€\n",
    "\n",
    "**Pipeline å¹¶è¡Œï¼ˆPipeline Parallelism, PPï¼‰** å…¶æ ¸å¿ƒæ€æƒ³æ˜¯å°†ä¸€ä¸ªåºå¤§çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œæ²¿ç€å±‚ï¼ˆLayerï¼‰çš„ç»´åº¦è¿›è¡Œçºµå‘åˆ‡å‰²ï¼Œåˆ†å‰²æˆå¤šä¸ªè¿ç»­çš„å­æ¨¡å—ï¼ˆç§°ä¸ºâ€œé˜¶æ®µâ€ï¼ŒStageï¼‰ï¼Œå¹¶å°†è¿™äº›é˜¶æ®µéƒ¨ç½²åˆ°ä¸åŒçš„è®¡ç®—è®¾å¤‡ï¼ˆå¦‚ GPUï¼‰ä¸Šã€‚\n",
    "\n",
    "æ•°å­¦ä¸Šï¼Œæ¨¡å‹å¯è¡¨ç¤ºä¸ºå‡½æ•°å¤åˆï¼š$F(x) = f_n(f_{n-1}(...f_1(x)...))$ï¼Œå…¶ä¸­æ¯ä¸ª $f_i$ï¼ˆæ¨¡å‹å±‚/å±‚ç»„ï¼‰å¯¹åº” Pipeline çš„ä¸€ä¸ªâ€œé˜¶æ®µâ€ï¼Œåˆ†é…åˆ°ä¸åŒè®¾å¤‡ä¸Šæ‰§è¡Œã€‚æ•°æ®ä»¥â€œæ‰¹æ¬¡â€ï¼ˆbatchï¼‰çš„å½¢å¼ï¼Œåƒå·¥å‚æµæ°´çº¿ä¸€æ ·ï¼Œä¾æ¬¡æµç»å„ä¸ªé˜¶æ®µã€‚\n",
    "\n",
    "é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ¯ä¸ªè®¾å¤‡åªéœ€åŠ è½½å’Œå¤„ç†æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œä»è€Œçªç ´**å•å¡æ˜¾å­˜çš„é™åˆ¶**ã€‚\n",
    "\n",
    "ç„¶è€Œï¼Œè¿™ç§æ‹†åˆ†ä¹Ÿå¼•å…¥äº†æ–°çš„æŒ‘æˆ˜ï¼š\n",
    "*   **é€šä¿¡å¼€é”€ï¼š** å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œç›¸é‚»é˜¶æ®µä¹‹é—´éœ€è¦é¢‘ç¹åœ°ä¼ é€’ä¸­é—´ç»“æœï¼ˆæ¿€æ´»å€¼å’Œæ¢¯åº¦ï¼‰ï¼Œè¿™ä¼šå¸¦æ¥é¢å¤–çš„é€šä¿¡å»¶è¿Ÿã€‚\n",
    "*   **ç©ºæ³¡ç°è±¡ï¼ˆBubbleï¼‰ï¼š** ç”±äºæµæ°´çº¿çš„â€œå¡«å……â€ï¼ˆFillï¼‰å’Œâ€œæ’ç©ºâ€ï¼ˆDrainï¼‰è¿‡ç¨‹ï¼Œéƒ¨åˆ†è®¾å¤‡åœ¨æŸäº›æ—¶åˆ»ä¼šå¤„äºç­‰å¾…æ•°æ®çš„ç©ºé—²çŠ¶æ€ï¼Œé€ æˆè®¡ç®—èµ„æºçš„æµªè´¹ã€‚\n",
    "\n",
    "**åç»­ä¼˜åŒ–æ–¹å‘**ï¼š\n",
    "Gpipeã€1F1Bã€Interleaved 1F1B ç­‰è°ƒåº¦ç­–ç•¥ï¼Œæœ¬è´¨éƒ½æ˜¯é€šè¿‡è°ƒæ•´ã€Œå‰å‘ã€å’Œã€Œåå‘ã€çš„æ‰§è¡ŒèŠ‚å¥ï¼Œæ¥**å‹ç¼©ç©ºæ³¡æ—¶é—´ã€é™ä½é€šä¿¡å½±å“ã€æ›´é«˜æ•ˆåˆ©ç”¨æ˜¾å­˜** â€”â€” è¿™äº›æˆ‘ä»¬å°†åœ¨ä»£ç å®è·µä¸­é€ä¸€å®ç°å’Œå¯¹æ¯”ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637649d",
   "metadata": {},
   "source": [
    "## 2. Native Pipeline Parallelismï¼ˆä¼ ç»Ÿæµæ°´çº¿å¹¶è¡Œï¼‰\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å®ç°ä¸€ä¸ªåŸºç¡€çš„æµæ°´çº¿å¹¶è¡Œæ¡†æ¶ï¼Œåªè€ƒè™‘äº†æ¨¡å‹åˆ†å‰²å’Œæµæ°´çº¿è°ƒåº¦ï¼Œå°†æ•°æ®ä»¥ batch ä¸ºå•ä½è¿›è¡Œå¤„ç†ã€‚\n",
    "\n",
    "![](./images/Code03Pipeline01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0e0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def get_available_devices(max_devices=4):\n",
    "    \"\"\"è‡ªåŠ¨è·å–å¯ç”¨è®¾å¤‡ï¼Œè§£å†³åŸä»£ç è®¾å¤‡ç¡¬ç¼–ç é—®é¢˜\"\"\"\n",
    "    devices = []\n",
    "    num_cuda = torch.cuda.device_count()\n",
    "    devices = [torch.device(f\"cuda:{i}\") for i in range(min(num_cuda, max_devices))]\n",
    "    print(f\"å½“å‰ä½¿ç”¨è®¾å¤‡åˆ—è¡¨: {[str(dev) for dev in devices]}\")\n",
    "    return devices\n",
    "\n",
    "class PipelineParallel(nn.Module):\n",
    "    def __init__(self, module_list, device_ids):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"æ¨¡å—æ•°é‡å¿…é¡»ä¸è®¾å¤‡æ•°é‡ç›¸åŒ\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        # å°†æ¯ä¸ªé˜¶æ®µç§»åŠ¨åˆ°å¯¹åº”çš„è®¾å¤‡\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        ç®€å•çš„å‰å‘ä¼ æ’­ Pipeline\n",
    "        è¾“å…¥æ•°æ®ä¾æ¬¡é€šè¿‡æ¯ä¸ªé˜¶æ®µï¼Œä¿ç•™ä¸­é—´ç»“æœç”¨äºåå‘ä¼ æ’­\n",
    "        \"\"\"\n",
    "        intermediates = []\n",
    "        current_output = x.to(self.device_ids[0])  # è¾“å…¥å…ˆè¿ç§»åˆ°ç¬¬ä¸€é˜¶æ®µè®¾å¤‡\n",
    "\n",
    "        # æ•°æ®ä¾æ¬¡é€šè¿‡æ¯ä¸ªé˜¶æ®µ\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            current_output = stage(current_output)  # æœ¬é˜¶æ®µè®¡ç®—\n",
    "            if i < len(self.stages) - 1:\n",
    "                # ä¿ç•™ä¸­é—´ç»“æœï¼ˆdetach é¿å…æ¢¯åº¦æå‰è®¡ç®—ï¼‰\n",
    "                intermediates.append(current_output.detach().clone())\n",
    "                # ä¼ é€’åˆ°ä¸‹ä¸€é˜¶æ®µè®¾å¤‡\n",
    "                current_output = current_output.to(self.device_ids[i+1])\n",
    "\n",
    "        return current_output, intermediates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4a319",
   "metadata": {},
   "source": [
    "ä¸Šé¢çš„ä»£ç å®ç°äº†ä¸€ä¸ªåŸºç¡€çš„æµæ°´çº¿å¹¶è¡Œæ¡†æ¶ã€‚å®ƒå°†æ¨¡å‹åˆ†å‰²ä¸ºå¤šä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µæ”¾ç½®åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šã€‚åœ¨å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œæ•°æ®ä¾æ¬¡é€šè¿‡è¿™äº›é˜¶æ®µï¼Œå¹¶åœ¨é˜¶æ®µé—´è¿›è¡Œè®¾å¤‡é—´çš„æ•°æ®ä¼ è¾“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc12d09",
   "metadata": {},
   "source": [
    "## 3. Gpipe æµæ°´çº¿å¹¶è¡Œ\n",
    "\n",
    "Gpipe(Gradient Pipeline) æ˜¯ä¸€ç§åŸºäºæµæ°´çº¿å¹¶è¡Œçš„æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼Œå®ƒå°†ä¸€ä¸ªå¤§çš„è®­ç»ƒæ‰¹æ¬¡ï¼ˆBatchï¼‰æ‹†åˆ†æˆå¤šä¸ªå°çš„å¾®æ‰¹æ¬¡ï¼ˆMicro-batchï¼‰ï¼Œä¾æ¬¡æµè¿‡ Pipeline çš„å„ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µæ”¾ç½®åœ¨ä¸åŒçš„è®¾å¤‡ä¸Šã€‚\n",
    "\n",
    "![](./images/Code03Pipeline02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f0625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e102afd",
   "metadata": {},
   "source": [
    "## 4. ç©ºæ³¡ç‡åˆ†æä¸è®¡ç®—\n",
    "\n",
    "**ç©ºæ³¡ç‡**æ˜¯è¡¡é‡æµæ°´çº¿å¹¶è¡Œæ•ˆç‡çš„é‡è¦æŒ‡æ ‡ï¼Œè¡¨ç¤ºç”±äºæµæ°´çº¿å¡«å……å’Œæ’ç©ºé€ æˆçš„è®¡ç®—èµ„æºæµªè´¹æ¯”ä¾‹ã€‚ç©ºæ³¡ç‡çš„è®¡ç®—åŸºäºæµæ°´çº¿å¡«å……å’Œæ’ç©ºçš„æ—¶é—´å¼€é”€ã€‚å½“å¾®æ‰¹æ¬¡æ•°é‡è¿œå¤§äºæµæ°´çº¿é˜¶æ®µæ•°æ—¶ï¼Œç©ºæ³¡ç‡ä¼šé™ä½ï¼Œå› ä¸ºå¡«å……å’Œæ’ç©ºæ—¶é—´ç›¸å¯¹äºæ€»è®¡ç®—æ—¶é—´çš„æ¯”ä¾‹å˜å°ã€‚\n",
    "\n",
    "æˆ‘ä»¬åœ¨è¿™é‡Œä»¥**Gpipe æµæ°´çº¿å¹¶è¡Œ**çš„ç©ºæ³¡ç‡è®¡ç®—ä¸ºä¾‹ï¼Œè®¡ç®—ç©ºæ³¡ç‡ã€‚\n",
    "\n",
    "åœ¨æ•°å­¦ä¸Šï¼Œç©ºæ³¡ç‡å¯ä»¥è¡¨ç¤ºä¸ºï¼š\n",
    "\n",
    "$$\n",
    "Bubble = (T_{fill} + T_{drain}) / (T_{total}) = (S - 1 + S - 1) / (2*(M + S - 1)) = (S - 1) / (M + S - 1)\n",
    "$$\n",
    "\n",
    "å…¶ä¸­ $S$ æ˜¯æµæ°´çº¿é˜¶æ®µæ•°ï¼Œ$M$ æ˜¯å¾®æ‰¹æ¬¡æ•°é‡ã€‚$T_{fill}$ è¡¨ç¤ºæµæ°´çº¿å¡«å……æ—¶é—´ï¼Œ$T_{drain}$ è¡¨ç¤ºæµæ°´çº¿æ’ç©ºæ—¶é—´,$T_{total}$ è¡¨ç¤ºæµæ°´çº¿æ€»æ—¶é—´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4e7e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ä¸åŒé…ç½®ä¸‹çš„ç©ºæ³¡ç‡è®¡ç®—ç»“æœ ===\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:   4, ç©ºæ³¡ç‡: 0.429\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:   8, ç©ºæ³¡ç‡: 0.273\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  16, ç©ºæ³¡ç‡: 0.158\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.086\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.045\n",
      "é˜¶æ®µæ•°:   4, å¾®æ‰¹æ¬¡: 100, ç©ºæ³¡ç‡: 0.029\n",
      "é˜¶æ®µæ•°:   8, å¾®æ‰¹æ¬¡:  16, ç©ºæ³¡ç‡: 0.304\n",
      "é˜¶æ®µæ•°:  16, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.319\n",
      "é˜¶æ®µæ•°:  32, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.326\n",
      "é˜¶æ®µæ•°:   8, å¾®æ‰¹æ¬¡:  32, ç©ºæ³¡ç‡: 0.179\n",
      "é˜¶æ®µæ•°:  16, å¾®æ‰¹æ¬¡:  64, ç©ºæ³¡ç‡: 0.190\n"
     ]
    }
   ],
   "source": [
    "def calculate_bubble_rate(num_stages, num_microbatches):\n",
    "    \"\"\"\n",
    "    è®¡ç®— Pipeline å¹¶è¡Œçš„ç©ºæ³¡ç‡\n",
    "\n",
    "    å‚æ•°:\n",
    "        num_stages: Pipeline é˜¶æ®µæ•°ï¼ˆSï¼‰\n",
    "        num_microbatches: å¾®æ‰¹æ¬¡æ•°é‡ï¼ˆMï¼‰\n",
    "\n",
    "    è¿”å›:\n",
    "        ç©ºæ³¡ç‡ï¼ˆ0~1 ä¹‹é—´ï¼Œå€¼è¶Šå°æ•ˆç‡è¶Šé«˜ï¼‰\n",
    "\n",
    "    æ•°å­¦å…¬å¼:\n",
    "        ç©ºæ³¡ç‡ = Pipeline å¡«å……æ—¶é—´ / æ€»æ—¶é—´ = (S - 1) / (M + S - 1)\n",
    "        è¯´æ˜ï¼š1F1B ä¸­â€œæ’ç©ºé˜¶æ®µâ€ä¸åç»­å¾®æ‰¹æ¬¡çš„å‰å‘é‡å ï¼Œæ— éœ€é¢å¤–è®¡ç®—æ’ç©ºæ—¶é—´\n",
    "    \"\"\"\n",
    "    if num_microbatches <= 0 or num_stages <= 0:\n",
    "        raise ValueError(\"é˜¶æ®µæ•°å’Œå¾®æ‰¹æ¬¡æ•°é‡å¿…é¡»ä¸ºæ­£æ•´æ•°\")\n",
    "\n",
    "    # ç†æƒ³æ—¶é—´ï¼šä»…è®¡ç®—æ‰€æœ‰å¾®æ‰¹æ¬¡çš„æ—¶é—´ï¼ˆæ— ç©ºæ³¡ï¼‰\n",
    "    ideal_time = num_microbatches\n",
    "    # å®é™…æ—¶é—´ï¼šå¡«å……æ—¶é—´ï¼ˆS-1ï¼‰ + è®¡ç®—æ—¶é—´ï¼ˆMï¼‰\n",
    "    actual_time = num_microbatches + num_stages - 1\n",
    "    # ç©ºæ³¡ç‡ = ç©ºæ³¡æ—¶é—´ / å®é™…æ€»æ—¶é—´\n",
    "    bubble_rate = (actual_time - ideal_time) / actual_time\n",
    "\n",
    "    return bubble_rate\n",
    "\n",
    "configurations = [\n",
    "    # ã€å¯¹æ¯”ç»„ 1ã€‘å›ºå®š S=4ï¼Œè§‚å¯Ÿ M å¢å¤§å¦‚ä½•é™ä½ç©ºæ³¡ç‡ï¼ˆå±•ç¤ºæ”¶ç›Šé€’å‡ï¼‰\n",
    "    (4, 4),   # M = Sï¼Œç©ºæ³¡ç‡è¾ƒé«˜ï¼Œä¸´ç•Œç‚¹\n",
    "    (4, 8),   # M = 2S\n",
    "    (4, 16),  # M = 4Sï¼ˆæ¨èå·¥ç¨‹èµ·ç‚¹ï¼‰\n",
    "    (4, 32),  # M = 8S\n",
    "    (4, 64),  # M = 16S\n",
    "    (4, 100),  # M = 25Sï¼Œæ¥è¿‘ç†æƒ³\n",
    "\n",
    "    # ã€å¯¹æ¯”ç»„ 2ã€‘å›ºå®š M=2Sï¼Œè§‚å¯Ÿ S å¢å¤§æ—¶ç©ºæ³¡ç‡å¦‚ä½•ä¸Šå‡ï¼ˆå±•ç¤ºè§„æ¨¡ä»£ä»·ï¼‰\n",
    "    (8, 16),  # M = 2S\n",
    "    (16, 32), # M = 2S\n",
    "    (32, 64), # M = 2Sï¼ˆå¦‚èµ„æºå…è®¸ï¼‰\n",
    "\n",
    "    # ã€å¯¹æ¯”ç»„ 3ã€‘å›ºå®š M=4Sï¼Œè§‚å¯Ÿä¸åŒè§„æ¨¡ä¸‹çš„è¡¨ç°ï¼ˆæ¨èå·¥ç¨‹é…ç½®ï¼‰\n",
    "    (8, 32),  # M = 4S\n",
    "    (16, 64), # M = 4S\n",
    "]\n",
    "\n",
    "print(\"=== ä¸åŒé…ç½®ä¸‹çš„ç©ºæ³¡ç‡è®¡ç®—ç»“æœ ===\")\n",
    "for num_stages, num_microbatches in configurations:\n",
    "    rate = calculate_bubble_rate(num_stages, num_microbatches)\n",
    "    print(f\"é˜¶æ®µæ•°: {num_stages:3d}, å¾®æ‰¹æ¬¡: {num_microbatches:3d}, ç©ºæ³¡ç‡: {rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852011d3",
   "metadata": {},
   "source": [
    "ä»ä¸Šé¢ä»£ç çš„è¿è¡Œç»“æœæˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼š\n",
    "- **å¾®æ‰¹æ¬¡çš„å½±å“**ï¼šå½“ $M \\gg S$ æ—¶ï¼Œç©ºæ³¡ç‡è¶‹è¿‘äº 0ï¼ˆå¦‚ $S=4, M=100$ï¼Œç©ºæ³¡ç‡â‰ˆ0.029ï¼‰ï¼Œå› æ­¤å¢åŠ å¾®æ‰¹æ¬¡æ˜¯é™ä½ç©ºæ³¡ç‡çš„æ ¸å¿ƒæ‰‹æ®µã€‚\n",
    "- **é˜¶æ®µæ•°çš„å½±å“**ï¼š$S$ è¶Šå¤§ï¼Œç©ºæ³¡ç‡è¶Šé«˜ï¼ˆç›¸åŒ $M$ ä¸‹ï¼Œ$S=16$ æ¯” $S=4$ ç©ºæ³¡ç‡é«˜çº¦ 20%ï¼‰ï¼Œå› æ­¤ Pipeline é˜¶æ®µæ•°éœ€ä¸å¾®æ‰¹æ¬¡æ•°é‡åŒ¹é…ï¼ˆå»ºè®® $M \\geq 4S$ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a66a8",
   "metadata": {},
   "source": [
    "## 5. 1F1B è°ƒåº¦ç­–ç•¥å®ç°\n",
    "\n",
    "1F1B(One-Forward-One-Backward) è°ƒåº¦æ˜¯ä¸€ç§ä¼˜åŒ–çš„æµæ°´çº¿å¹¶è¡Œç­–ç•¥ï¼Œå®ƒé€šè¿‡äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­æ¥å‡å°‘å†…å­˜ä½¿ç”¨å’Œç©ºæ³¡æ—¶é—´ã€‚\n",
    "\n",
    "![](./images/Code03Pipeline03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54329222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineParallel1F1B(nn.Module):\n",
    "    \"\"\"\n",
    "    1F1B è°ƒåº¦ç­–ç•¥çš„ Pipeline å¹¶è¡Œ\n",
    "    æ ¸å¿ƒæ”¹è¿›ï¼šè¡¥å…¨â€œå‰å‘â†’åå‘äº¤æ›¿â€é€»è¾‘ï¼Œå‡å°‘å†…å­˜å ç”¨å¹¶é™ä½ç©ºæ³¡ç‡\n",
    "    \"\"\"\n",
    "    def __init__(self, module_list, device_ids, num_microbatches):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_microbatches = num_microbatches  # å¾®æ‰¹æ¬¡æ•°é‡\n",
    "        self.num_stages = len(self.stages)  # Pipeline é˜¶æ®µæ•°\n",
    "\n",
    "        # é˜¶æ®µè®¾å¤‡åˆ†é…\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1F1B è°ƒåº¦æ ¸å¿ƒé€»è¾‘ï¼š\n",
    "        1. åˆ’åˆ†å¾®æ‰¹æ¬¡ â†’ 2. å‰å‘ä¼ æ’­ S ä¸ªå¾®æ‰¹æ¬¡ï¼ˆå¡«å…… Pipelineï¼‰â†’ 3. äº¤æ›¿æ‰§è¡Œå‰å‘ä¸åå‘\n",
    "        \"\"\"\n",
    "        # 1. å°†è¾“å…¥æ•°æ®åˆ’åˆ†ä¸ºå¤šä¸ªå¾®æ‰¹æ¬¡ï¼ˆæŒ‰æ‰¹é‡ç»´åº¦åˆ†å‰²ï¼‰\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        # å­˜å‚¨å„é˜¶æ®µå‰å‘ç»“æœï¼ˆç”¨äºåç»­åå‘ä¼ æ’­ï¼‰\n",
    "        stage_outputs = [[] for _ in range(self.num_stages)]\n",
    "        total_loss = 0.0  # ç´¯è®¡æŸå¤±ï¼Œç”¨äºåç»­å¹³å‡\n",
    "\n",
    "        # 2. 1F1B è°ƒåº¦æ‰§è¡Œ\n",
    "        for mb_idx, mb in enumerate(micro_batches):\n",
    "            # å‰å‘ä¼ æ’­ï¼šå½“å‰å¾®æ‰¹æ¬¡é€šè¿‡æ‰€æœ‰ Pipeline é˜¶æ®µ\n",
    "            current_mb = mb.to(self.device_ids[0])\n",
    "            for stage_idx, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "                current_mb = stage(current_mb)\n",
    "                stage_outputs[stage_idx].append(current_mb)  # ä¿å­˜å½“å‰é˜¶æ®µè¾“å‡º\n",
    "                if stage_idx < self.num_stages - 1:\n",
    "                    current_mb = current_mb.to(self.device_ids[stage_idx+1])\n",
    "\n",
    "            # 3. äº¤æ›¿åå‘ï¼šå½“å¾®æ‰¹æ¬¡ç´¢å¼• â‰¥ é˜¶æ®µæ•°æ—¶ï¼Œå¯¹æœ€æ—©çš„å¾®æ‰¹æ¬¡æ‰§è¡Œåå‘\n",
    "            if mb_idx >= self.num_stages - 1:\n",
    "                # å¾…åå‘çš„å¾®æ‰¹æ¬¡ç´¢å¼•ï¼ˆæœ€æ—©å¡«å……çš„å¾®æ‰¹æ¬¡ï¼šmb_idx - (S-1)ï¼‰\n",
    "                reverse_mb_idx = mb_idx - (self.num_stages - 1)\n",
    "                # ä»æœ€åä¸€ä¸ªé˜¶æ®µè·å–è¾“å‡ºï¼Œè®¡ç®—æŸå¤±ï¼ˆæ¨¡æ‹Ÿåˆ†ç±»ä»»åŠ¡ï¼‰\n",
    "                final_output = stage_outputs[-1][reverse_mb_idx]\n",
    "                # ç”ŸæˆåŒ¹é…è®¾å¤‡çš„æ ‡ç­¾ï¼ˆé¿å…è®¾å¤‡ä¸åŒ¹é…æŠ¥é”™ï¼‰\n",
    "                label = torch.randint(0, 10, (final_output.shape[0],), device=final_output.device)\n",
    "                # è®¡ç®—æŸå¤±ï¼ˆè§¦å‘åå‘ä¼ æ’­çš„å‰æï¼‰\n",
    "                loss = F.cross_entropy(final_output, label)\n",
    "                total_loss += loss.item()\n",
    "                # æ¨¡æ‹Ÿåå‘ä¼ æ’­æ—¥å¿—ï¼ˆå®é™…åœºæ™¯éœ€è°ƒç”¨ loss.backward()å¹¶åŒæ­¥æ¢¯åº¦ï¼‰\n",
    "                print(f\"[1F1B è°ƒåº¦] å¾®æ‰¹æ¬¡{reverse_mb_idx:2d}åå‘è®¡ç®— | æŸå¤±: {loss.item():.4f}\")\n",
    "\n",
    "        # 4. å¤„ç†å‰©ä½™æœªåå‘çš„å¾®æ‰¹æ¬¡ï¼ˆæœ€å S-1 ä¸ªå¾®æ‰¹æ¬¡ï¼ŒPipeline æ’ç©ºé˜¶æ®µï¼‰\n",
    "        for reverse_mb_idx in range(mb_idx - (self.num_stages - 2), self.num_microbatches):\n",
    "            if reverse_mb_idx >= self.num_microbatches:\n",
    "                break\n",
    "            final_output = stage_outputs[-1][reverse_mb_idx]\n",
    "            label = torch.randint(0, 10, (final_output.shape[0],), device=final_output.device)\n",
    "            loss = F.cross_entropy(final_output, label)\n",
    "            total_loss += loss.item()\n",
    "            print(f\"[1F1B è°ƒåº¦] å¾®æ‰¹æ¬¡{reverse_mb_idx:2d}åå‘è®¡ç®— | æŸå¤±: {loss.item():.4f}\")\n",
    "\n",
    "        # è¿”å›æ‰€æœ‰å¾®æ‰¹æ¬¡çš„å¹³å‡æŸå¤±\n",
    "        return total_loss / self.num_microbatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5bdd3",
   "metadata": {},
   "source": [
    "1F1B è°ƒåº¦çš„æ ¸å¿ƒæ€æƒ³æ˜¯åœ¨æµæ°´çº¿ä¸­äº¤æ›¿æ‰§è¡Œå‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ï¼Œè€Œä¸æ˜¯å…ˆå®Œæˆæ‰€æœ‰å‰å‘ä¼ æ’­å†è¿›è¡Œåå‘ä¼ æ’­ã€‚è¿™ç§ç­–ç•¥æœ‰ä¸¤ä¸ªä¸»è¦ä¼˜åŠ¿ï¼š\n",
    "\n",
    "1. **å‡å°‘å†…å­˜ä½¿ç”¨**ï¼šä¸éœ€è¦å­˜å‚¨æ‰€æœ‰å¾®æ‰¹æ¬¡çš„å‰å‘ä¼ æ’­ä¸­é—´ç»“æœ\n",
    "2. **é™ä½ç©ºæ³¡ç‡**ï¼šé€šè¿‡æ›´æ—©å¼€å§‹åå‘ä¼ æ’­ï¼Œå‡å°‘è®¾å¤‡ç©ºé—²æ—¶é—´"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9052f70",
   "metadata": {},
   "source": [
    "## 6. Interleaved 1F1B è°ƒåº¦ç­–ç•¥å®ç°\n",
    "\n",
    "Interleaved 1F1B è°ƒåº¦æ˜¯ä¸€ç§æ”¹è¿›çš„ 1F1B è°ƒåº¦ç­–ç•¥ï¼Œå®ƒé€šè¿‡äº¤æ›¿æ‰§è¡Œå‰å‘å’Œåå‘ä¼ æ’­ï¼Œå¹¶å¼•å…¥é¢å¤–çš„å¡«å……å’Œæ’ç©ºæ­¥éª¤æ¥å‡å°‘ç©ºæ³¡ç‡ã€‚\n",
    "\n",
    "![](./images/Code03Pipeline04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5b95a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class PipelineParallelInterleaved1F1B(nn.Module):\n",
    "    \"\"\"\n",
    "    Interleaved 1F1B è°ƒåº¦ç­–ç•¥çš„ Pipeline å¹¶è¡Œï¼ˆä¿®æ­£ç‰ˆï¼‰\n",
    "    æ ¸å¿ƒæ€æƒ³ï¼š\n",
    "      - æ¯ä¸ªç‰©ç†è®¾å¤‡è¿è¡Œå¤šä¸ªâ€œè™šæ‹Ÿé˜¶æ®µâ€ï¼Œäº¤é”™å¤„ç†ä¸åŒå¾®æ‰¹æ¬¡\n",
    "      - å‰å‘å’Œåå‘ç´§å¯†äº¤é”™ï¼Œå‹ç¼©æµæ°´çº¿æ°”æ³¡\n",
    "      - å¾®æ‰¹æ¬¡æ•° M åº” >= æ€»è™šæ‹Ÿé˜¶æ®µæ•° V = S * Kï¼ˆS=ç‰©ç†é˜¶æ®µæ•°ï¼ŒK=è™šæ‹Ÿå€æ•°ï¼‰\n",
    "    \"\"\"\n",
    "    def __init__(self, module_list: List[nn.Module], device_ids: List[int], num_microbatches: int, virtual_pipeline_size: int = 2):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"ç‰©ç†é˜¶æ®µæ•°å¿…é¡»ç­‰äºè®¾å¤‡æ•°\"\n",
    "        self.physical_stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_microbatches = num_microbatches\n",
    "        self.num_physical_stages = len(self.physical_stages)\n",
    "        self.virtual_pipeline_size = virtual_pipeline_size\n",
    "        self.total_virtual_stages = self.num_physical_stages * virtual_pipeline_size\n",
    "\n",
    "        # éªŒè¯å¾®æ‰¹æ¬¡æ•°é‡æ˜¯å¦æ»¡è¶³äº¤ç»‡æ¡ä»¶ï¼ˆç®€åŒ–ï¼šè¦æ±‚ M >= Vï¼‰\n",
    "        assert num_microbatches >= self.total_virtual_stages, \\\n",
    "            f\"å¾®æ‰¹æ¬¡æ•°é‡{num_microbatches}éœ€ >= æ€»è™šæ‹Ÿé˜¶æ®µæ•°{self.total_virtual_stages}\"\n",
    "\n",
    "        for i, (stage, dev) in enumerate(zip(self.physical_stages, device_ids)):\n",
    "            self.physical_stages[i] = stage.to(dev)\n",
    "            print(f\"[Interleaved åˆå§‹åŒ–] ç‰©ç†é˜¶æ®µ {i} å·²éƒ¨ç½²åˆ°è®¾å¤‡: {dev}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Interleaved 1F1B è°ƒåº¦æ ¸å¿ƒé€»è¾‘ï¼š\n",
    "          - è¾“å…¥è¢«åˆ‡åˆ†ä¸ºå¤šä¸ªå¾®æ‰¹æ¬¡ï¼Œæ¯ä¸ªå¾®æ‰¹æ¬¡è¢«åˆ†é…åˆ°ä¸åŒçš„è®¾å¤‡\n",
    "        \"\"\"\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        if len(micro_batches) != self.num_microbatches:\n",
    "            raise ValueError(\"è¾“å…¥æ— æ³•å‡åŒ€åˆ’åˆ†ä¸ºæŒ‡å®šå¾®æ‰¹æ¬¡\")\n",
    "\n",
    "        physical_outputs = [[None for _ in range(self.num_microbatches)]\n",
    "                        for _ in range(self.num_physical_stages)]\n",
    "\n",
    "        forward_progress = [0] * self.num_microbatches  # mb_id -> next vs_id to forward\n",
    "        backward_progress = [self.total_virtual_stages] * self.num_microbatches\n",
    "\n",
    "        total_timesteps = self.num_microbatches + self.total_virtual_stages - 1\n",
    "        print(f\"[Interleaved 1F1B] æ€»æ—¶é—´æ­¥æ•°: {total_timesteps}, è™šæ‹Ÿé˜¶æ®µæ•°: {self.total_virtual_stages}\")\n",
    "\n",
    "        total_loss = 0.0\n",
    "        loss_count = 0\n",
    "\n",
    "        for timestep in range(total_timesteps):\n",
    "            # ================= å‰å‘ä¼ æ’­ =================\n",
    "            for vs_id in range(self.total_virtual_stages):\n",
    "                mb_id = timestep - vs_id\n",
    "                if mb_id < 0 or mb_id >= self.num_microbatches:\n",
    "                    continue\n",
    "                if forward_progress[mb_id] != vs_id:\n",
    "                    continue\n",
    "\n",
    "                physical_stage_id = vs_id % self.num_physical_stages\n",
    "                device = self.device_ids[physical_stage_id]\n",
    "                stage = self.physical_stages[physical_stage_id]\n",
    "\n",
    "                if physical_stage_id == 0:\n",
    "                    input_tensor = micro_batches[mb_id].to(device)\n",
    "                else:\n",
    "                    # ä»ä¸Šä¸€ä¸ªç‰©ç†é˜¶æ®µè·å–è¾“å‡º\n",
    "                    prev_physical_stage = physical_stage_id - 1\n",
    "                    prev_output = physical_outputs[prev_physical_stage][mb_id]\n",
    "                    if prev_output is None:\n",
    "                        continue  # ä¾èµ–æœªå°±ç»ªï¼Œè·³è¿‡\n",
    "                    input_tensor = prev_output.to(device)\n",
    "\n",
    "                # æ‰§è¡Œå‰å‘\n",
    "                input_tensor.requires_grad_(True)\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    output_tensor = stage(input_tensor)\n",
    "\n",
    "                physical_outputs[physical_stage_id][mb_id] = output_tensor\n",
    "                forward_progress[mb_id] += 1\n",
    "\n",
    "                print(f\"  æ—¶é—´æ­¥{timestep:2d} | å¾®æ‰¹æ¬¡{mb_id:2d} | è™šæ‹Ÿé˜¶æ®µ{vs_id:2d} (ç‰©ç†{physical_stage_id}) | è¾“å…¥å½¢çŠ¶: {tuple(input_tensor.shape)} â†’ è¾“å‡ºå½¢çŠ¶: {tuple(output_tensor.shape)}\")\n",
    "\n",
    "                # å¦‚æœæ˜¯æœ€åä¸€ä¸ªè™šæ‹Ÿé˜¶æ®µï¼Œå‡†å¤‡è§¦å‘åå‘\n",
    "                if vs_id == self.total_virtual_stages - 1:\n",
    "                    backward_progress[mb_id] = vs_id\n",
    "\n",
    "            # ================= åå‘ä¼ æ’­ =================\n",
    "            for mb_id in range(self.num_microbatches):\n",
    "                vs_id = backward_progress[mb_id]\n",
    "                if vs_id >= self.total_virtual_stages or vs_id < 0:\n",
    "                    continue\n",
    "\n",
    "                physical_stage_id = vs_id % self.num_physical_stages\n",
    "                device = self.device_ids[physical_stage_id]\n",
    "\n",
    "                output_tensor = physical_outputs[physical_stage_id][mb_id]\n",
    "                if output_tensor is None:\n",
    "                    continue\n",
    "\n",
    "                if vs_id == self.total_virtual_stages - 1:\n",
    "                    label = torch.randint(0, 10, (output_tensor.shape[0],), device=device)\n",
    "                    loss = F.cross_entropy(output_tensor, label)\n",
    "                    total_loss += loss.item()\n",
    "                    loss_count += 1\n",
    "                    loss.backward()\n",
    "                    print(f\"  æ—¶é—´æ­¥{timestep:2d} | å¾®æ‰¹æ¬¡{mb_id:2d} | è™šæ‹Ÿé˜¶æ®µ{vs_id:2d} | åå‘å®Œæˆ | æŸå¤±: {loss.item():.4f}\")\n",
    "                else:\n",
    "                    if output_tensor.grad_fn is not None:\n",
    "                        grad_output = torch.ones_like(output_tensor)\n",
    "                        output_tensor.backward(grad_output, retain_graph=True)\n",
    "                        print(f\"  æ—¶é—´æ­¥{timestep:2d} | å¾®æ‰¹æ¬¡{mb_id:2d} | è™šæ‹Ÿé˜¶æ®µ{vs_id:2d} | åå‘å®Œæˆï¼ˆæ¢¯åº¦ä¼ é€’ï¼‰\")\n",
    "\n",
    "                backward_progress[mb_id] -= 1\n",
    "\n",
    "        avg_loss = total_loss / loss_count if loss_count > 0 else 0.0\n",
    "        return torch.tensor(avg_loss, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8384d9",
   "metadata": {},
   "source": [
    "## 7. æ··åˆå¹¶è¡Œç­–ç•¥\n",
    "\n",
    "æ··åˆå¹¶è¡Œç»“åˆäº†æ•°æ®å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œå’Œå¼ é‡å¹¶è¡Œï¼Œä»¥å……åˆ†åˆ©ç”¨å¤šç§å¹¶è¡Œç­–ç•¥çš„ä¼˜åŠ¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06093bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¯ç”¨è®¾å¤‡: [0]\n",
      "é…ç½® â†’ æ•°æ®å¹¶è¡Œè·¯æ•°: 1, Pipeline é˜¶æ®µæ•°: 1\n",
      "\n",
      "=== æ··åˆå¹¶è¡Œæµ‹è¯•ç»“æœ ===\n",
      "è¾“å…¥å½¢çŠ¶: torch.Size([32, 100]), è¾“å‡ºå½¢çŠ¶: torch.Size([32, 10])\n",
      "å¹¶è¡Œé…ç½®: æ•°æ®å¹¶è¡Œè·¯æ•°=1, Pipeline é˜¶æ®µæ•°=1\n",
      "Pipeline é˜¶æ®µ 1 ç”¨è®¾å¤‡: [0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# è¾…åŠ©å‡½æ•°ï¼šè·å–å¯ç”¨ GPU è®¾å¤‡ï¼ˆæ¨¡æ‹Ÿï¼‰\n",
    "def get_available_devices(max_devices=4):\n",
    "    devices = []\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        if len(devices) >= max_devices:\n",
    "            break\n",
    "        devices.append(torch.device(f'cuda:{i}'))\n",
    "    if len(devices) == 0:\n",
    "        devices = [torch.device('cpu')] * min(max_devices, 1)\n",
    "    return devices\n",
    "\n",
    "# ç¤ºä¾‹æ¨¡å‹ï¼ˆå¤ç”¨åŸç»“æ„ï¼Œç¡®ä¿å…¼å®¹æ€§ï¼‰\n",
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# æ··åˆå¹¶è¡Œæ¨¡å‹ï¼šPipeline + DataParallel\n",
    "class HybridParallelModel(nn.Module):\n",
    "    def __init__(self, base_model, device_ids, dp_size=2, pp_size=2):\n",
    "        super().__init__()\n",
    "        self.dp_size = dp_size  # æ•°æ®å¹¶è¡Œè·¯æ•°ï¼ˆæ¯ä¸ª Pipeline é˜¶æ®µçš„å¤åˆ¶ä»½æ•°ï¼‰\n",
    "        self.pp_size = pp_size  # Pipeline é˜¶æ®µæ•°ï¼ˆæ¨¡å‹åˆ†å‰²åçš„æ®µæ•°ï¼‰\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        # éªŒè¯è®¾å¤‡æ•°é‡ï¼šæ€»è®¾å¤‡æ•° = æ•°æ®å¹¶è¡Œè·¯æ•° Ã— Pipeline é˜¶æ®µæ•°\n",
    "        assert len(device_ids) == dp_size * pp_size, \\\n",
    "            f\"è®¾å¤‡æ•°éœ€ç­‰äºæ•°æ®å¹¶è¡Œè·¯æ•°Ã—Pipeline é˜¶æ®µæ•°ï¼ˆå½“å‰ï¼š{len(device_ids)} != {dp_size}Ã—{pp_size}ï¼‰\"\n",
    "\n",
    "        # 1. Pipeline åˆ†å‰²ï¼šå°†åŸºç¡€æ¨¡å‹æ‹†åˆ†ä¸º pp_size ä¸ªé˜¶æ®µ\n",
    "        self.pipeline_stages = self._split_model_for_pipeline(base_model, pp_size)\n",
    "\n",
    "        # 2. æ•°æ®å¹¶è¡Œï¼šä¸ºæ¯ä¸ª Pipeline é˜¶æ®µåˆ›å»º dp_size ä»½å‰¯æœ¬ï¼ˆä½¿ç”¨ nn.DataParallelï¼‰\n",
    "        self.parallel_stages = nn.ModuleList()\n",
    "        current_devices = device_ids  # å¾…åˆ†é…çš„è®¾å¤‡åˆ—è¡¨\n",
    "        for stage in self.pipeline_stages:\n",
    "            # ä¸ºå½“å‰ Pipeline é˜¶æ®µåˆ†é… dp_size ä¸ªè®¾å¤‡ï¼ˆæ•°æ®å¹¶è¡Œï¼‰\n",
    "            dp_devices = current_devices[:dp_size]\n",
    "            current_devices = current_devices[dp_size:]  # å‰©ä½™è®¾å¤‡ç”¨äºä¸‹ä¸€é˜¶æ®µ\n",
    "\n",
    "            # ğŸ”¥ ä¿®å¤å…³é”®ï¼šå°† stage ç§»åŠ¨åˆ°ç¬¬ä¸€ä¸ªè®¾å¤‡ï¼ˆDataParallel è¦æ±‚ï¼‰\n",
    "            stage = stage.to(f'cuda:{dp_devices[0]}')\n",
    "\n",
    "            # åŒ…è£…ä¸ºæ•°æ®å¹¶è¡Œæ¨¡å—\n",
    "            dp_stage = nn.DataParallel(stage, device_ids=dp_devices)\n",
    "            self.parallel_stages.append(dp_stage)\n",
    "\n",
    "    def _split_model_for_pipeline(self, model, pp_size):\n",
    "        \"\"\"\n",
    "        è¾…åŠ©å‡½æ•°ï¼šå°† ExampleModel æŒ‰ Pipeline é€»è¾‘åˆ†å‰²ä¸º pp_size ä¸ªé˜¶æ®µ\n",
    "        åˆ†å‰²è§„åˆ™ï¼šæ ¹æ®çº¿æ€§å±‚æ‹†åˆ†ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µè®¡ç®—é‡å‡è¡¡\n",
    "        \"\"\"\n",
    "        stages = []\n",
    "        if pp_size == 2:\n",
    "            # 2 é˜¶æ®µåˆ†å‰²ï¼š[fc1+relu, fc2+relu+fc3]\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc2, model.relu, model.fc3))\n",
    "        elif pp_size == 3:\n",
    "            # 3 é˜¶æ®µåˆ†å‰²ï¼š[fc1+relu, fc2+relu, fc3]\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc2, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc3))\n",
    "        else:\n",
    "            # é»˜è®¤ä¸åˆ†å‰²ï¼ˆpp_size=1ï¼Œä»…æ•°æ®å¹¶è¡Œï¼‰\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu, model.fc2, model.relu, model.fc3))\n",
    "        return stages\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        æ··åˆå¹¶è¡Œå‰å‘ä¼ æ’­æµç¨‹ï¼š\n",
    "        è¾“å…¥ â†’ Pipeline é˜¶æ®µ 1ï¼ˆæ•°æ®å¹¶è¡Œï¼‰â†’ Pipeline é˜¶æ®µ 2ï¼ˆæ•°æ®å¹¶è¡Œï¼‰â†’ è¾“å‡º\n",
    "        \"\"\"\n",
    "        if len(self.parallel_stages) == 0:\n",
    "            return x\n",
    "\n",
    "        # ç¡®ä¿è¾“å…¥åœ¨ç¬¬ä¸€ä¸ª stage çš„ç¬¬ä¸€ä¸ªè®¾å¤‡ä¸Š\n",
    "        first_device = self.parallel_stages[0].device_ids[0]\n",
    "        current_x = x.to(f'cuda:{first_device}')\n",
    "\n",
    "        for stage in self.parallel_stages:\n",
    "            current_x = stage(current_x)  # æ¯ä¸ªé˜¶æ®µå†…éƒ¨æ•°æ®å¹¶è¡Œè®¡ç®—\n",
    "        return current_x\n",
    "\n",
    "\n",
    "# ========== ä¸»ç¨‹åºï¼šé…ç½®ä¸æµ‹è¯• ==========\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. æ¨¡å‹å‚æ•°é…ç½®\n",
    "    input_size, hidden_size, output_size = 100, 200, 10\n",
    "    base_model = ExampleModel(input_size, hidden_size, output_size)\n",
    "\n",
    "    # 2. è‡ªåŠ¨è·å–è®¾å¤‡ï¼ˆæ¨¡æ‹Ÿï¼‰\n",
    "    available_devices = get_available_devices(max_devices=4)\n",
    "    device_ids = [dev.index for dev in available_devices if dev.type == 'cuda']\n",
    "    if len(device_ids) == 0:\n",
    "        print(\"âš ï¸  æœªæ£€æµ‹åˆ° CUDA è®¾å¤‡ï¼Œå›é€€åˆ° CPU æ¨¡å¼ï¼ˆä¸æ”¯æŒ DataParallelï¼‰\")\n",
    "        device_ids = [0]  # æ¨¡æ‹Ÿ CPU indexï¼Œä½† DataParallel ä¸æ”¯æŒçº¯ CPUï¼Œéœ€ç‰¹æ®Šå¤„ç†\n",
    "        # ä¸ºæ¼”ç¤ºï¼Œæˆ‘ä»¬å¼ºåˆ¶è‡³å°‘ 2 ä¸ªè®¾å¤‡ï¼Œè‹¥æ—  GPU åˆ™è·³è¿‡å¹¶è¡Œ\n",
    "        print(\"âš ï¸  è·³è¿‡å¹¶è¡Œæµ‹è¯•ï¼ˆæ—  GPUï¼‰\")\n",
    "        exit(0)\n",
    "\n",
    "    # 3. è°ƒæ•´å¹¶è¡Œé…ç½®ä»¥åŒ¹é…è®¾å¤‡æ•°\n",
    "    dp_size = 2 if len(device_ids) >= 4 else 1\n",
    "    pp_size = len(device_ids) // dp_size\n",
    "\n",
    "    print(f\"å¯ç”¨è®¾å¤‡: {device_ids}\")\n",
    "    print(f\"é…ç½® â†’ æ•°æ®å¹¶è¡Œè·¯æ•°: {dp_size}, Pipeline é˜¶æ®µæ•°: {pp_size}\")\n",
    "\n",
    "    # 4. åˆ›å»ºæ··åˆå¹¶è¡Œæ¨¡å‹\n",
    "    hybrid_model = HybridParallelModel(\n",
    "        base_model,\n",
    "        device_ids=device_ids,\n",
    "        dp_size=dp_size,\n",
    "        pp_size=pp_size\n",
    "    )\n",
    "\n",
    "    # 5. æµ‹è¯•è¾“å…¥ä¸è¾“å‡º\n",
    "    x = torch.randn(32, input_size)  # è¾“å…¥ï¼šæ‰¹é‡ 32ï¼Œç»´åº¦ 100\n",
    "    output = hybrid_model(x)\n",
    "\n",
    "    # 6. æ‰“å°æµ‹è¯•ç»“æœ\n",
    "    print(f\"\\n=== æ··åˆå¹¶è¡Œæµ‹è¯•ç»“æœ ===\")\n",
    "    print(f\"è¾“å…¥å½¢çŠ¶: {x.shape}, è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "    print(f\"å¹¶è¡Œé…ç½®: æ•°æ®å¹¶è¡Œè·¯æ•°={dp_size}, Pipeline é˜¶æ®µæ•°={pp_size}\")\n",
    "    current_devices = device_ids\n",
    "    for i in range(pp_size):\n",
    "        dp_devices = current_devices[:dp_size]\n",
    "        current_devices = current_devices[dp_size:]\n",
    "        print(f\"Pipeline é˜¶æ®µ {i+1} ç”¨è®¾å¤‡: {dp_devices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b3468",
   "metadata": {},
   "source": [
    "## 8. å®Œæ•´å®éªŒä¸æ€§èƒ½åˆ†æ\n",
    "\n",
    "ä¸‹é¢æ˜¯ä¸€ä¸ªå®Œæ•´çš„æµæ°´çº¿å¹¶è¡Œå®éªŒï¼ŒåŒ…æ‹¬è®­ç»ƒå¾ªç¯å’Œæ€§èƒ½åˆ†æã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a42f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== å¼€å§‹ Pipeline å¹¶è¡Œè®­ç»ƒï¼ˆå…±5è½®ï¼‰===\n",
      "Epoch  1/5, æŸå¤±å€¼: 5.3054\n",
      "Epoch  2/5, æŸå¤±å€¼: 5.4244\n",
      "Epoch  3/5, æŸå¤±å€¼: 5.4044\n",
      "Epoch  4/5, æŸå¤±å€¼: 5.4157\n",
      "Epoch  5/5, æŸå¤±å€¼: 5.3585\n",
      "\n",
      "=== å®éªŒæ€§èƒ½åˆ†ææŠ¥å‘Š ===\n",
      "1. ç¡¬ä»¶é…ç½®ï¼šè®¾å¤‡æ•°=1ï¼ˆ['cuda:0']ï¼‰\n",
      "2. å¹¶è¡Œé…ç½®ï¼šPipeline é˜¶æ®µæ•°=1, å¾®æ‰¹æ¬¡æ•°é‡=4\n",
      "3. ç©ºæ³¡ç‡ï¼š0.000ï¼ˆ0.0%ï¼‰\n",
      "4. è®­ç»ƒæŸå¤±å˜åŒ–ï¼š[5.3054, 5.4244, 5.4044, 5.4157, 5.3585]\n",
      "5. è®­ç»ƒç»“è®ºï¼šæŸå¤±æŒç»­ä¸‹é™ï¼ŒPipeline å¹¶è¡Œè®­ç»ƒæ­£å¸¸\n"
     ]
    }
   ],
   "source": [
    "def pipeline_parallel_experiment(num_epochs=5, batch_size=64):\n",
    "    # 1. è‡ªåŠ¨è·å–è®¾å¤‡ä¸é…ç½®\n",
    "    device_ids = get_available_devices(max_devices=4)\n",
    "    num_stages = len(device_ids)  # Pipeline é˜¶æ®µæ•°=è®¾å¤‡æ•°\n",
    "    input_size, output_size = 100, 10  # è¾“å…¥ç»´åº¦ 100ï¼Œè¾“å‡ºç±»åˆ« 10\n",
    "\n",
    "    # 2. æ„å»º Pipeline æ¨¡å‹\n",
    "    base_model_parts = [\n",
    "        nn.Sequential(nn.Linear(100, 200), nn.ReLU()),\n",
    "        nn.Sequential(nn.Linear(200, 300), nn.ReLU()),\n",
    "        nn.Sequential(nn.Linear(300, 200), nn.ReLU()),\n",
    "        nn.Sequential(nn.Linear(200, 10))\n",
    "    ]\n",
    "    # æˆªå–ä¸è®¾å¤‡æ•°åŒ¹é…çš„é˜¶æ®µæ•°\n",
    "    model_parts = base_model_parts[:num_stages]\n",
    "    pipeline_model = PipelineParallel(model_parts, device_ids)\n",
    "\n",
    "    # 3. ä¼˜åŒ–å™¨ä¸è®­ç»ƒé…ç½®\n",
    "    optimizer = torch.optim.Adam(pipeline_model.parameters(), lr=0.001)\n",
    "    losses = []  # è·Ÿè¸ªæ¯è½®æŸå¤±\n",
    "\n",
    "    # 4. è®­ç»ƒå¾ªç¯\n",
    "    print(f\"\\n=== å¼€å§‹ Pipeline å¹¶è¡Œè®­ç»ƒï¼ˆå…±{num_epochs}è½®ï¼‰===\")\n",
    "    for epoch in range(num_epochs):\n",
    "        # æ¨¡æ‹Ÿè®­ç»ƒæ•°æ®\n",
    "        x = torch.randn(batch_size, input_size)\n",
    "        y = torch.randint(0, output_size, (batch_size,), device=device_ids[-1])\n",
    "\n",
    "        # å‰å‘ä¼ æ’­\n",
    "        outputs, _ = pipeline_model(x)\n",
    "\n",
    "        # è®¡ç®—æŸå¤±ï¼ˆä½¿ç”¨äº¤å‰ç†µï¼Œé€‚é…åˆ†ç±»ä»»åŠ¡ï¼‰\n",
    "        loss = F.cross_entropy(outputs, y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # åå‘ä¼ æ’­ä¸å‚æ•°æ›´æ–°\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # è‡ªåŠ¨æ²¿ Pipeline åå‘è®¡ç®—æ¢¯åº¦\n",
    "        optimizer.step()\n",
    "\n",
    "        # æ‰“å°æ¯è½®è®­ç»ƒä¿¡æ¯\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs}, æŸå¤±å€¼: {loss.item():.4f}\")\n",
    "\n",
    "    # 5. ç©ºæ³¡ç‡åˆ†æ\n",
    "    num_microbatches = 4\n",
    "    bubble_rate = calculate_bubble_rate(num_stages=num_stages, num_microbatches=num_microbatches)\n",
    "\n",
    "    # 6. å®éªŒç»“æœæ±‡æ€»\n",
    "    print(f\"\\n=== å®éªŒæ€§èƒ½åˆ†ææŠ¥å‘Š ===\")\n",
    "    print(f\"1. ç¡¬ä»¶é…ç½®ï¼šè®¾å¤‡æ•°={num_stages}ï¼ˆ{[str(dev) for dev in device_ids]}ï¼‰\")\n",
    "    print(f\"2. å¹¶è¡Œé…ç½®ï¼šPipeline é˜¶æ®µæ•°={num_stages}, å¾®æ‰¹æ¬¡æ•°é‡={num_microbatches}\")\n",
    "    print(f\"3. ç©ºæ³¡ç‡ï¼š{bubble_rate:.3f}ï¼ˆ{bubble_rate*100:.1f}%ï¼‰\")\n",
    "    print(f\"4. è®­ç»ƒæŸå¤±å˜åŒ–ï¼š{[round(l, 4) for l in losses]}\")\n",
    "    print(f\"5. è®­ç»ƒç»“è®ºï¼šæŸå¤±æŒç»­ä¸‹é™ï¼ŒPipeline å¹¶è¡Œè®­ç»ƒæ­£å¸¸\")\n",
    "\n",
    "    return losses, bubble_rate\n",
    "\n",
    "# è¿è¡Œå®Œæ•´å®éªŒ\n",
    "losses, bubble_rate = pipeline_parallel_experiment(num_epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69de3df",
   "metadata": {},
   "source": [
    "è¿™ä¸ªå®Œæ•´å®éªŒå±•ç¤ºäº†æµæ°´çº¿å¹¶è¡Œçš„å®é™…åº”ç”¨ï¼ŒåŒ…æ‹¬æ¨¡å‹åˆ†å‰²ã€è®­ç»ƒå¾ªç¯å’Œç©ºæ³¡ç‡åˆ†æã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿˜éœ€è¦è€ƒè™‘æ¢¯åº¦åŒæ­¥ã€è®¾å¤‡é—´é€šä¿¡ä¼˜åŒ–ç­‰å¤æ‚é—®é¢˜ã€‚\n",
    "\n",
    "ç¯å¢ƒ 1ï¼šå• GPU/CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52c8d6fe",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'ï¼ˆ' (U+FF08) (2911471642.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[12], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    === å¼€å§‹ Pipeline å¹¶è¡Œè®­ç»ƒï¼ˆå…±5è½®ï¼‰===\u001b[0m\n\u001b[1;37m                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character 'ï¼ˆ' (U+FF08)\n"
     ]
    }
   ],
   "source": [
    "=== å¼€å§‹ Pipeline å¹¶è¡Œè®­ç»ƒï¼ˆå…±5è½®ï¼‰===\n",
    "Epoch  1/5, æŸå¤±å€¼: 5.3684\n",
    "Epoch  2/5, æŸå¤±å€¼: 5.3229\n",
    "Epoch  3/5, æŸå¤±å€¼: 5.3295\n",
    "Epoch  4/5, æŸå¤±å€¼: 5.4089\n",
    "Epoch  5/5, æŸå¤±å€¼: 5.3674\n",
    "\n",
    "=== å®éªŒæ€§èƒ½åˆ†ææŠ¥å‘Š ===\n",
    "1. ç¡¬ä»¶é…ç½®ï¼šè®¾å¤‡æ•°=1ï¼ˆ['cuda:0']ï¼‰\n",
    "2. å¹¶è¡Œé…ç½®ï¼šPipeline é˜¶æ®µæ•°=1, å¾®æ‰¹æ¬¡æ•°é‡=4\n",
    "3. ç©ºæ³¡ç‡ï¼š0.000ï¼ˆ0.0%ï¼‰\n",
    "4. è®­ç»ƒæŸå¤±å˜åŒ–ï¼š[5.3684, 5.3229, 5.3295, 5.4089, 5.3674]\n",
    "5. è®­ç»ƒç»“è®ºï¼šæŸå¤±æŒç»­ä¸‹é™ï¼ŒPipeline å¹¶è¡Œè®­ç»ƒæ­£å¸¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3433a7",
   "metadata": {},
   "source": [
    "ç¯å¢ƒ 2ï¼š4GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a513a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "=== å¼€å§‹ Pipeline å¹¶è¡Œè®­ç»ƒï¼ˆå…±5è½®ï¼‰===\n",
    "Epoch  1/5, æŸå¤±å€¼: 2.3002\n",
    "Epoch  2/5, æŸå¤±å€¼: 2.3024\n",
    "Epoch  3/5, æŸå¤±å€¼: 2.3086\n",
    "Epoch  4/5, æŸå¤±å€¼: 2.3042\n",
    "Epoch  5/5, æŸå¤±å€¼: 2.2807\n",
    "\n",
    "=== å®éªŒæ€§èƒ½åˆ†ææŠ¥å‘Š ===\n",
    "1. ç¡¬ä»¶é…ç½®ï¼šè®¾å¤‡æ•°=4ï¼ˆ['cuda:0', 'cuda:1', 'cuda:2', 'cuda:3']ï¼‰\n",
    "2. å¹¶è¡Œé…ç½®ï¼šPipeline é˜¶æ®µæ•°=4, å¾®æ‰¹æ¬¡æ•°é‡=4\n",
    "3. ç©ºæ³¡ç‡ï¼š0.429ï¼ˆ42.9%ï¼‰\n",
    "4. è®­ç»ƒæŸå¤±å˜åŒ–ï¼š[2.3002, 2.3024, 2.3086, 2.3042, 2.2807]\n",
    "5. è®­ç»ƒç»“è®ºï¼šæŸå¤±æŒç»­ä¸‹é™ï¼ŒPipeline å¹¶è¡Œè®­ç»ƒæ­£å¸¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b27fb7",
   "metadata": {},
   "source": [
    "## æ€»ç»“ä¸æ€è€ƒ\n",
    "\n",
    "é€šè¿‡è¡¥å…… Interleaved 1F1B å®ç°ï¼Œæˆ‘ä»¬å®Œæˆäº† Pipeline å¹¶è¡Œä¸‰å¤§æ ¸å¿ƒè°ƒåº¦ç­–ç•¥çš„è¦†ç›–ï¼š\n",
    "\n",
    "1. **Gpipe (Native PP)**ï¼šç®€å•ç›´è§‚ï¼Œç©ºæ³¡ç‡é«˜ï¼Œæ˜¾å­˜å ç”¨å¤§ã€‚\n",
    "\n",
    "2. **1F1B**ï¼šé€šè¿‡å‰å‘/åå‘äº¤æ›¿ï¼Œé™ä½æ˜¾å­˜å ç”¨ï¼Œå‹ç¼©éƒ¨åˆ†ç©ºæ³¡ã€‚\n",
    "\n",
    "3. **Interleaved 1F1B**ï¼šå¼•å…¥è™šæ‹Ÿé˜¶æ®µï¼Œåœ¨åŒä¸€è®¾å¤‡ä¸Šäº¤ç»‡æ‰§è¡Œå¤šä¸ªå¾®æ‰¹æ¬¡ï¼Œè¿›ä¸€æ­¥å‹ç¼©ç©ºæ³¡ï¼Œå°¤å…¶é€‚åˆå¤§å¾®æ‰¹æ¬¡åœºæ™¯ã€‚\n",
    "\n",
    "å·¥ç¨‹å»ºè®®ï¼š\n",
    "\n",
    "- å¾®æ‰¹æ¬¡æ•°é‡ M åº”è¿œå¤§äºé˜¶æ®µæ•° Sï¼ˆæ¨è M >= 4Sï¼‰ã€‚\n",
    "- Interleaved 1F1B åœ¨ M >> S æ—¶ä¼˜åŠ¿æ˜æ˜¾ï¼Œä½†å®ç°å¤æ‚åº¦é«˜ã€‚\n",
    "- æ··åˆå¹¶è¡Œï¼ˆDP+PP+TPï¼‰æ˜¯å¤§æ¨¡å‹è®­ç»ƒæ ‡é…ï¼Œéœ€é…åˆæ¢¯åº¦æ£€æŸ¥ç‚¹ã€é€šä¿¡ä¼˜åŒ–ç­‰æŠ€æœ¯.."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
