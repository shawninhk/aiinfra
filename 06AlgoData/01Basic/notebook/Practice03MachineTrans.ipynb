{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb02e00",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# 实战 Transformer 机器翻译(DONE)\n",
    "\n",
    "author by: ZOMI\n",
    "\n",
    "本次将把之前实现的 Transformer 模型应用于真实的机器翻译任务，使用 [IWSLT 2016 英德数据集](https://www.kaggle.com/datasets/tttzof351/iwslt-2016-de-en)。该数据集包含英德双语平行语料，句子长度适中（多为日常对话或短文本），适合验证 Transformer 在中低资源翻译任务中的效果。\n",
    "\n",
    "我们将引入一些训练过程的最佳实践，包括学习率调度、标签平滑、梯度裁剪等优化技巧——这些技术是解决 Transformer 训练不稳定性（如梯度爆炸、过拟合）和提升泛化能力的核心手段，并实现贪婪搜索和束搜索算法进行推理解码（两种算法分别平衡推理速度与翻译质量），最后使用 BLEU 分数评估翻译质量（机器翻译领域的标准自动评估指标）。\n",
    "\n",
    "## 1. 环境准备与数据加载\n",
    "\n",
    "首先，我们导入必要的库并设置环境。这里需要重点说明：PyTorch 的随机种子设置（`torch.manual_seed`等）是为了确保实验可重现——Transformer 模型参数规模大，随机初始化的微小差异可能导致训练结果波动，固定种子后可排除随机因素对实验结论的干扰。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e3114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from torchtext.datasets import IWSLT2016\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True  # 禁用 CuDNN 的非确定性算法，进一步保证可重现性\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c55282",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "使用设备: cuda\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e35117",
   "metadata": {},
   "source": [
    "### 1.1 加载和预处理数据\n",
    "\n",
    "我们将使用 torchtext 库加载 IWSLT 2016 英德翻译数据集，并进行预处理。数据预处理是机器翻译的关键步骤，核心目标是将原始文本转换为模型可处理的数值序列，同时保留语言的语义和结构信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda7e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载英语和德语的 spacy 模型用于分词\n",
    "try:\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "except OSError:\n",
    "    # 如果还没有下载模型，先下载\n",
    "    print(\"正在下载 spacy 模型...\")\n",
    "    import os\n",
    "    os.system(\"python -m spacy download en_core_web_sm\")\n",
    "    os.system(\"python -m spacy download de_core_news_sm\")\n",
    "    spacy_en = spacy.load('en_core_web_sm')\n",
    "    spacy_de = spacy.load('de_core_news_sm')\n",
    "\n",
    "# 定义分词函数\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    英语分词函数\n",
    "    \"\"\"\n",
    "    return [token.text for token in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    德语分词函数\n",
    "    \"\"\"\n",
    "    return [token.text for token in spacy_de.tokenizer(text)]\n",
    "\n",
    "# 定义 Field 对象处理文本\n",
    "# Field 负责文本的预处理逻辑：分词、添加边界符号、小写化、数值化等\n",
    "SRC = Field(tokenize=tokenize_de, \n",
    "            init_token='<sos>',  # 序列起始符号，让模型识别翻译的开始\n",
    "            eos_token='<eos>',  # 序列结束符号，让模型识别翻译的结束\n",
    "            lower=True,  # 小写化统一大小写，减少词汇表规模（如\"The\"和\"the\"视为同一词）\n",
    "            batch_first=True)  # 输出张量格式为[batch_size, seq_len]，符合 PyTorch 常用习惯\n",
    "\n",
    "TRG = Field(tokenize=tokenize_en, \n",
    "            init_token='<sos>', \n",
    "            eos_token='<eos>', \n",
    "            lower=True,\n",
    "            batch_first=True)\n",
    "\n",
    "# 加载 IWSLT2016 数据集\n",
    "# splits 函数按语言后缀(.de 为德语源语言，.en 为英语目标语言)划分数据，并关联 Field 处理逻辑\n",
    "print(\"加载 IWSLT2016 数据集...\")\n",
    "train_data, valid_data, test_data = IWSLT2016.splits(exts=('.de', '.en'), \n",
    "                                                     fields=(SRC, TRG))\n",
    "\n",
    "# 构建词汇表\n",
    "# 基于训练集统计词频，min_freq=2 表示只保留出现次数≥2 的词，过滤低频噪声词\n",
    "print(\"构建词汇表...\")\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "\n",
    "print(f\"源语言词汇表大小: {len(SRC.vocab)}\")\n",
    "print(f\"目标语言词汇表大小: {len(TRG.vocab)}\")\n",
    "\n",
    "# 创建数据迭代器\n",
    "# BucketIterator 按序列长度分组（同批次句子长度相近），减少 padding 数量，提升计算效率\n",
    "BATCH_SIZE = 128\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n",
    "\n",
    "print(\"数据加载完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04998c8",
   "metadata": {},
   "source": [
    "对应输出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1461041",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "加载 IWSLT2016 数据集...\n",
    "构建词汇表...\n",
    "源语言词汇表大小: 18832\n",
    "目标语言词汇表大小: 35432\n",
    "数据加载完成!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3522f33c",
   "metadata": {},
   "source": [
    "## 2. 模型构建与优化技术\n",
    "\n",
    "### 2.1 构建 Transformer 模型\n",
    "\n",
    "我们将使用之前实现的 Transformer 模型，但进行一些调整以适应机器翻译任务。尽管我们使用了较小的模型（D_MODEL=256，N_LAYERS=3），Transformer 仍然能够学习英德翻译的基本模式，生成语法基本正确的翻译结果——这得益于 Transformer 的自注意力机制，能有效捕捉语言的长距离依赖（如德语的后置定语与英语的前置定语对应关系）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入之前实现的 Transformer 组件\n",
    "from transformer_components import Embedding, PositionalEncoding, MultiHeadAttention\n",
    "from transformer_components import FeedForward, SublayerConnection, EncoderLayer\n",
    "from transformer_components import DecoderLayer, Encoder, Decoder, Transformer, Generator\n",
    "\n",
    "def make_model(src_vocab_size, trg_vocab_size, d_model=512, N=6, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    构建完整的 Transformer 模型\n",
    "    \n",
    "    Args:\n",
    "        src_vocab_size: 源语言词汇表大小\n",
    "        trg_vocab_size: 目标语言词汇表大小\n",
    "        d_model: 模型核心维度（所有子层输入输出维度统一为 d_model，确保特征传递一致性）\n",
    "        N: 编码器/解码器层数（多层叠加可捕捉更复杂的语言结构，如短语级、句子级依赖）\n",
    "        d_ff: 前馈网络内部维度（通常为 d_model 的 4 倍，通过维度扩张增强特征表达能力）\n",
    "        h: 注意力头数（多头注意力将 d_model 拆分 h 份，并行捕捉不同类型的依赖，如语义、语法）\n",
    "        dropout: Dropout 率（防止过拟合，在注意力层和前馈层随机丢弃部分特征）\n",
    "        \n",
    "    Returns:\n",
    "        完整的 Transformer 模型\n",
    "    \"\"\"\n",
    "    # 创建注意力机制和前馈网络\n",
    "    attn = MultiHeadAttention(d_model, h, dropout)\n",
    "    ff = FeedForward(d_model, d_ff, dropout)\n",
    "    \n",
    "    # 创建位置编码\n",
    "    # Transformer 无循环结构，需通过位置编码注入序列的时序信息（如\"我吃饭\"和\"饭吃我\"的语序差异）\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = Transformer(\n",
    "        Encoder(EncoderLayer(d_model, attn, ff, dropout), N),  # 编码器：多层 EncoderLayer 堆叠，处理源语言序列\n",
    "        Decoder(DecoderLayer(d_model, attn, attn, ff, dropout), N),  # 解码器：多层 DecoderLayer 堆叠，生成目标语言序列\n",
    "        nn.Sequential(Embedding(src_vocab_size, d_model), deepcopy(position)),  # 源语言嵌入+位置编码\n",
    "        nn.Sequential(Embedding(trg_vocab_size, d_model), deepcopy(position)),  # 目标语言嵌入+位置编码\n",
    "        Generator(d_model, trg_vocab_size)  # 生成器：将 d_model 维度特征映射到目标词汇表概率分布\n",
    "    )\n",
    "    \n",
    "    # 初始化参数\n",
    "    # Xavier 均匀初始化适用于线性层，确保前向传播特征方差与反向传播梯度方差一致，避免梯度消失/爆炸\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "            \n",
    "    return model\n",
    "\n",
    "# 创建模型\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "D_MODEL = 256  # 为了训练速度，使用较小的模型（标准 Transformer 为 512）\n",
    "N_LAYERS = 3  # 层数减少（标准为 6），平衡模型能力与训练成本\n",
    "HID_DIM = 512  # 前馈网络内部维度（通常为 d_model 的 2 倍，此处适配小模型）\n",
    "N_HEADS = 8  # 保持 8 头注意力，确保多类型依赖捕捉能力\n",
    "DROPOUT = 0.1  # 适度 Dropout，缓解小模型过拟合\n",
    "\n",
    "model = make_model(INPUT_DIM, OUTPUT_DIM, D_MODEL, N_LAYERS, HID_DIM, N_HEADS, DROPOUT).to(device)\n",
    "\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5da90dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "模型参数量: 25,634,336\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f3f666",
   "metadata": {},
   "source": [
    "### 2.2 标签平滑 (Label Smoothing)\n",
    "\n",
    "标签平滑是一种正则化技术，通过软化硬标签来防止模型过度自信，提高泛化能力。在机器翻译中，硬标签（one-hot 编码）会让模型对\"正确词\"的预测概率趋近于 1，对其他词趋近于 0，导致模型对微小输入变化敏感（如源语言中一个词的歧义），泛化到测试集时容易出错。\n",
    "\n",
    "**原理公式**：\n",
    "\n",
    "$$\n",
    "y_{\\text{smooth}} = (1 - \\epsilon) \\cdot y + \\frac{\\epsilon}{K}\n",
    "$$\n",
    "\n",
    "其中 $y$ 是原始 one-hot 标签（正确词位置为 1，其余为 0），$\\epsilon$ 是平滑因子（通常取 0.1，控制软化程度），$K$ 是目标词汇表大小（类别数）。公式含义是：给正确词保留 $(1-\\epsilon)$ 的概率，剩余 $\\epsilon$ 均匀分配给其他所有词，迫使模型学习更鲁棒的特征（不仅能区分正确词，还能理解其他词的合理性）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a46f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    标签平滑实现\n",
    "    \n",
    "    Args:\n",
    "        smoothing: 平滑因子（ε）\n",
    "        pad_idx: 填充索引（不应用平滑，因填充 token 无实际语义，不应参与损失计算）\n",
    "    \"\"\"\n",
    "    def __init__(self, smoothing=0.1, pad_idx=0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.pad_idx = pad_idx\n",
    "        self.confidence = 1.0 - smoothing  # 正确词的保留概率\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')  # KL 散度损失：衡量预测分布与平滑后目标分布的差异\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 模型输出（log 概率，shape: [batch_size*seq_len, trg_vocab_size]）\n",
    "            target: 目标标签（原始 token 索引，shape: [batch_size*seq_len]）\n",
    "            \n",
    "        Returns:\n",
    "            平滑后的损失（按批次大小归一化，确保不同批次损失可比）\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        x = x.contiguous().view(-1, x.size(-1))  # 展平为[总 token 数, 词汇表大小]\n",
    "        target = target.contiguous().view(-1)  # 展平为[总 token 数]\n",
    "        \n",
    "        # 创建平滑后的目标分布\n",
    "        true_dist = x.clone()\n",
    "        # 给所有非 pad 词分配ε/(K-2)（减去 pad 和正确词两类）\n",
    "        true_dist.fill_(self.smoothing / (x.size(1) - 2))\n",
    "        \n",
    "        # 将正确词的位置概率设为(1-ε)\n",
    "        true_dist.scatter_(1, target.unsqueeze(1), self.confidence)\n",
    "        \n",
    "        # 将 pad 位置的概率设为 0（不参与损失）\n",
    "        true_dist[:, self.pad_idx] = 0\n",
    "        mask = target == self.pad_idx\n",
    "        if mask.sum() > 0:\n",
    "            true_dist.index_fill_(0, mask.nonzero().squeeze(), 0.0)\n",
    "            \n",
    "        return self.criterion(x, true_dist.detach()) / batch_size  # 按批次大小归一化\n",
    "\n",
    "# 创建标签平滑损失函数\n",
    "criterion = LabelSmoothing(smoothing=0.1, pad_idx=TRG.vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9006ae",
   "metadata": {},
   "source": [
    "### 2.3 学习率调度 (Learning Rate Scheduling)\n",
    "\n",
    "Transformer 使用带 warmup 的学习率调度策略，先线性增加学习率，然后按步数的平方根反比衰减。这一策略是为了解决 Transformer 训练的两个核心问题：1）训练初期参数随机，小学习率避免参数震荡；2）训练后期参数接近最优，小学习率微调避免过拟合。\n",
    "\n",
    "**原理公式**：\n",
    "\n",
    "$$\n",
    "\\text{lrate} = d_{\\text{model}}^{-0.5} \\cdot \\min(\\text{step_num}^{-0.5}, \\text{step_num} \\cdot \\text{warmup_steps}^{-1.5})\n",
    "$$\n",
    "\n",
    "公式解读：\n",
    "\n",
    "- $d_{\\text{model}}^{-0.5}$：模型维度越大，初始学习率越小——因大模型参数更多，需更谨慎的更新幅度；\n",
    "- $\\text{step_num} \\cdot \\text{warmup_steps}^{-1.5}$（warmup 阶段）：学习率随步数线性增加，直到 warmup_steps 时达到峰值；\n",
    "- $\\text{step_num}^{-0.5}$（warmup 后）：学习率随步数平方根反比衰减，确保后期更新幅度逐渐减小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548e3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerOptimizer:\n",
    "    \"\"\"\n",
    "    Transformer 专用的优化器，包含学习率调度\n",
    "    \n",
    "    Args:\n",
    "        optimizer: 基础优化器（此处用 Adam，适合大参数模型的自适应优化）\n",
    "        d_model: 模型维度（用于计算初始学习率缩放因子）\n",
    "        warmup_steps: warmup 步数（通常取 4000，平衡训练初期稳定性与收敛速度）\n",
    "        factor: 学习率整体缩放因子（微调学习率范围）\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000, factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "        self.step_num = 0  # 记录训练步数\n",
    "        self.lr = 0  # 记录当前学习率\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        更新参数和学习率：先计算当前学习率，再更新优化器参数\n",
    "        \"\"\"\n",
    "        self.step_num += 1\n",
    "        lr = self._get_lr()\n",
    "        # 给所有参数组设置当前学习率\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = lr\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        清空梯度：PyTorch 默认梯度累积，需手动清空\n",
    "        \"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "    def _get_lr(self):\n",
    "        \"\"\"\n",
    "        计算当前学习率（按公式实现）\n",
    "        \"\"\"\n",
    "        lr = self.factor * (self.d_model ** -0.5) * \\\n",
    "             min(self.step_num ** -0.5, self.step_num * self.warmup_steps ** -1.5)\n",
    "        self.lr = lr\n",
    "        return lr\n",
    "\n",
    "# 创建优化器和学习率调度器\n",
    "# Adam 优化器的 betas=(0.9, 0.98)：一阶矩（动量）用 0.9 加速收敛，二阶矩用 0.98 关注近期梯度变化\n",
    "# eps=1e-9：避免分母为 0，适合 Transformer 的大参数规模\n",
    "optimizer = TransformerOptimizer(\n",
    "    optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9),\n",
    "    d_model=D_MODEL,\n",
    "    warmup_steps=4000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b184264",
   "metadata": {},
   "source": [
    "### 2.4 梯度裁剪 (Gradient Clipping)\n",
    "\n",
    "梯度裁剪可以防止训练过程中梯度爆炸问题，提高训练稳定性。Transformer 的残差连接虽能缓解梯度消失，但多层叠加（即使是 3 层）仍可能导致部分参数的梯度 norms 过大，进而导致参数更新幅度过大，模型震荡甚至发散。\n",
    "\n",
    "梯度裁剪的核心逻辑：计算所有可训练参数梯度的 L2 范数（整体梯度规模），若超过预设的 max_norm，则按比例（max_norm / 实际范数）缩放所有梯度，确保梯度在合理范围内。例如 max_norm=1.0 是常见阈值，既允许足够的更新幅度，又避免梯度爆炸。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340305a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradients(model, max_norm=1.0):\n",
    "    \"\"\"\n",
    "    梯度裁剪\n",
    "    \n",
    "    Args:\n",
    "        model: 模型（需裁剪所有可训练参数的梯度）\n",
    "        max_norm: 最大梯度范数（阈值，通常取 1.0 或 5.0）\n",
    "    \"\"\"\n",
    "    # 计算所有参数的梯度 L2 范数（平方和开根号）\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:  # 仅处理有梯度的参数（如冻结层无梯度）\n",
    "            param_norm = p.grad.data.norm(2)  # 单个参数的梯度 L2 范数\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm ** (1. / 2)\n",
    "    \n",
    "    # 裁剪梯度：若总范数超过 max_norm，按比例缩放\n",
    "    clip_coef = max_norm / (total_norm + 1e-6)  # +1e-6 避免分母为 0\n",
    "    if clip_coef < 1:  # 仅当总范数超过阈值时裁剪\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                p.grad.data.mul_(clip_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2938a57",
   "metadata": {},
   "source": [
    "## 3. 训练与验证\n",
    "\n",
    "### 3.1 训练循环\n",
    "\n",
    "训练循环的核心是实现 Transformer 的前向传播、损失计算、反向传播和参数更新，其中**掩码机制**是确保 Transformer 正确工作的关键——需分别处理源序列的 pad 掩码和目标序列的自回归掩码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "    训练一个 epoch（遍历一次训练集）\n",
    "    \n",
    "    Args:\n",
    "        model: 模型\n",
    "        iterator: 数据迭代器（训练集）\n",
    "        optimizer: 优化器（含学习率调度）\n",
    "        criterion: 损失函数（标签平滑）\n",
    "        clip: 梯度裁剪阈值\n",
    "        \n",
    "    Returns:\n",
    "        平均损失（按迭代器长度归一化，反映该 epoch 的训练效果）\n",
    "    \"\"\"\n",
    "    model.train()  # 设为训练模式：启用 Dropout、BatchNorm 更新等\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src  # 源语言序列，shape: [batch_size, src_seq_len]\n",
    "        trg = batch.trg  # 目标语言序列，shape: [batch_size, trg_seq_len]\n",
    "        \n",
    "        # 创建掩码：解决 pad token 和自回归问题\n",
    "        # 1. 源序列掩码（src_mask）：忽略 pad token 的注意力计算\n",
    "        # 形状从[batch_size, src_seq_len]扩展为[batch_size, 1, 1, src_seq_len]，适配多头注意力的维度\n",
    "        src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "        \n",
    "        # 2. 目标序列掩码（trg_mask）：包含 pad 掩码和上三角掩码（自回归）\n",
    "        trg_pad_mask = (trg != TRG.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "        # 上三角掩码（nopeak_mask）：防止解码器看到未来的词（如生成第 i 个词时，看不到 i+1 及以后的词）\n",
    "        nopeak_mask = torch.triu(torch.ones(trg_pad_mask.size(0), trg_pad_mask.size(1), trg_pad_mask.size(2)), \n",
    "                                diagonal=1).to(device) == 0\n",
    "        trg_mask = trg_pad_mask & nopeak_mask  # 合并两种掩码\n",
    "        \n",
    "        optimizer.zero_grad()  # 清空梯度（避免累积）\n",
    "        \n",
    "        # 前向传播：解码器输入为 trg[:, :-1]（去掉最后一个词），输出预测 trg[:, 1:]（去掉第一个词）\n",
    "        # 原因：自回归生成中，用前 i 个词预测第 i+1 个词，需对齐输入输出\n",
    "        output = model(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        \n",
    "        # 计算损失：输出 shape [batch_size, trg_seq_len-1, trg_vocab_size]，目标 shape [batch_size, trg_seq_len-1]\n",
    "        loss = criterion(output, trg[:, 1:])\n",
    "        \n",
    "        # 反向传播：计算梯度\n",
    "        loss.backward()\n",
    "        \n",
    "        # 梯度裁剪：防止梯度爆炸\n",
    "        clip_gradients(model, clip)\n",
    "        \n",
    "        # 更新参数：含学习率调度\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()  # 累积损失\n",
    "        \n",
    "        # 每 100 批次打印一次中间结果，监控训练进度\n",
    "        if i % 100 == 0:\n",
    "            print(f\"批次 {i}, 损失: {loss.item():.4f}, 学习率: {optimizer.lr:.6f}\")\n",
    "            \n",
    "    return epoch_loss / len(iterator)  # 返回平均损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02560219",
   "metadata": {},
   "source": [
    "### 3.2 验证循环\n",
    "\n",
    "验证循环与训练循环逻辑相似，但需关闭梯度计算（节省显存、加速推理），且不更新参数、不启用 Dropout，确保验证结果反映模型的泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3966041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    验证模型（遍历一次验证集或测试集）\n",
    "    \n",
    "    Args:\n",
    "        model: 模型\n",
    "        iterator: 数据迭代器（验证集/测试集）\n",
    "        criterion: 损失函数（标签平滑）\n",
    "        \n",
    "    Returns:\n",
    "        平均损失（反映模型的泛化能力）\n",
    "    \"\"\"\n",
    "    model.eval()  # 设为评估模式：禁用 Dropout、固定 BatchNorm 等\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():  # 关闭梯度计算，节省显存和计算资源\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            # 创建掩码（逻辑与训练一致）\n",
    "            src_mask = (src != SRC.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "            trg_mask = (trg != TRG.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "            nopeak_mask = torch.triu(torch.ones(trg_mask.size(0), trg_mask.size(1), trg_mask.size(2)), \n",
    "                                    diagonal=1).to(device) == 0\n",
    "            trg_mask = trg_mask & nopeak_mask\n",
    "            \n",
    "            # 前向传播（逻辑与训练一致）\n",
    "            output = model(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(output, trg[:, 1:])      \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "    return epoch_loss / len(iterator)  # 返回平均损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65f3b9",
   "metadata": {},
   "source": [
    "### 3.3 训练模型\n",
    "\n",
    "训练过程中需监控训练损失和验证损失：若验证损失下降，说明模型泛化能力提升；若验证损失上升，可能出现过拟合，需提前停止训练（此处通过保存\"最佳模型\"实现，即仅保存验证损失最低的模型参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e00774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "N_EPOCHS = 10  # 小模型训练 10 个 epoch 足够收敛，大模型需更多 epoch\n",
    "CLIP = 1.0  # 梯度裁剪阈值\n",
    "best_valid_loss = float('inf')  # 初始化最佳验证损失为无穷大\n",
    "\n",
    "# 训练模型\n",
    "print(\"开始训练模型...\")\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)  # 计算 epoch 耗时\n",
    "    \n",
    "    # 保存最佳模型：仅当当前验证损失低于历史最佳时保存，避免保存过拟合模型\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "    \n",
    "    # 打印 epoch 结果：训练损失下降说明模型在学习，验证损失下降说明泛化能力提升\n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.4f}')\n",
    "    print(f'\\tVal Loss: {valid_loss:.4f}')\n",
    "    \n",
    "print(\"训练完成!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c173fd8",
   "metadata": {},
   "source": [
    "在训练过程中，你将看到类似以下的输出。随着训练的进行，训练损失和验证损失应该逐渐下降，表明模型在学习翻译任务。若训练损失持续下降但验证损失上升，说明模型过拟合，需调整 Dropout 率或减小模型规模。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba703037",
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "开始训练模型...\n",
    "批次 0, 损失: 10.4253, 学习率: 0.000002\n",
    "批次 100, 损失: 6.8321, 学习率: 0.000052\n",
    "...\n",
    "Epoch: 01 | Time: 3m 45s\n",
    "    Train Loss: 6.2345\n",
    "    Val Loss: 5.8901\n",
    "...\n",
    "Epoch: 10 | Time: 3m 42s\n",
    "    Train Loss: 3.1245\n",
    "    Val Loss: 4.2310\n",
    "训练完成!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cba79ca",
   "metadata": {},
   "source": [
    "## 4. 推理解码算法\n",
    "\n",
    "训练完成后，需通过**解码算法**将模型的输出（词汇表概率分布）转换为可读的目标语言序列。常用的解码算法有贪婪搜索和束搜索，二者在速度和翻译质量上存在权衡。\n",
    "\n",
    "### 4.1 贪婪搜索 (Greedy Search)\n",
    "\n",
    "贪婪搜索在每一步选择概率最高的词作为当前输出，优点是速度快（每步仅需一次 argmax 操作），缺点是容易陷入**局部最优**——例如某一步选择概率最高的词，但后续无法组成语义通顺的句子（如德语\"Können Sie\"翻译时，第一步选\"Can\"是最优，但第二步若选\"you\"而非\"you please\"，可能导致后续翻译不完整）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b08713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    贪婪搜索解码\n",
    "    \n",
    "    Args:\n",
    "        model: 模型\n",
    "        src: 源序列（单句，shape: [1, src_seq_len]）\n",
    "        src_mask: 源序列掩码\n",
    "        max_len: 最大生成长度（防止无限生成，通常设为源序列长度的 1.5~2 倍）\n",
    "        start_symbol: 开始符号索引（<sos>的数值化表示）\n",
    "        \n",
    "    Returns:\n",
    "        解码后的序列（shape: [1, trg_seq_len]，包含<sos>和<eos>）\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 编码源序列：得到源语言的特征表示 memory（shape: [1, src_seq_len, d_model]）\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    # 初始化目标序列：从<sos>开始，shape: [1, 1]\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    \n",
    "    for i in range(max_len-1):  # 减去初始的<sos>，避免超过 max_len\n",
    "        # 创建目标序列掩码：包含 pad 掩码和上三角掩码（自回归）\n",
    "        trg_mask = (ys != TRG.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "        nopeak_mask = torch.triu(torch.ones(trg_mask.size(0), trg_mask.size(1), trg_mask.size(2)), \n",
    "                                diagonal=1).to(device) == 0\n",
    "        trg_mask = trg_mask & nopeak_mask\n",
    "        \n",
    "        # 解码：用当前目标序列 ys 和源特征 memory 预测下一个词\n",
    "        out = model.decode(memory, src_mask, ys, trg_mask)  # out shape: [1, len(ys), d_model]\n",
    "        prob = model.generator(out[:, -1])  # 取最后一个词的特征，映射为词汇表概率（shape: [1, trg_vocab_size]）\n",
    "        _, next_word = torch.max(prob, dim=1)  # 贪婪选择概率最高的词（argmax）\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        # 将下一个词添加到目标序列\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        \n",
    "        # 如果遇到<eos>，停止生成（序列已完整）\n",
    "        if next_word == TRG.vocab.stoi['<eos>']:\n",
    "            break\n",
    "            \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4c3e2",
   "metadata": {},
   "source": [
    "### 4.2 束搜索 (Beam Search)\n",
    "\n",
    "**解码策略比较**：束搜索（beam search）通常比贪婪搜索（greedy search）能产生质量更高的翻译结果，因为它考虑了更多可能的翻译路径——贪婪搜索每步仅保留 1 个候选序列，束搜索每步保留`beam_size`个候选序列（如 beam_size=5），通过多路径探索避免局部最优；但束搜索速度较慢（候选数越多，计算量越大），需在速度和质量间权衡。\n",
    "\n",
    "束搜索的核心逻辑：1）初始化`beam_size`个候选序列（均从<sos>开始）；2）每步对每个候选序列扩展所有可能的下一个词，计算序列的累积概率；3）保留概率最高的`beam_size`个候选序列；4）重复步骤 2-3，直到所有序列生成<eos>或达到 max_len；5）应用**长度惩罚**（避免生成过短序列），选择最优序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode(model, src, src_mask, max_len, start_symbol, beam_size, length_penalty=0.6):\n",
    "    \"\"\"\n",
    "    束搜索解码\n",
    "    \n",
    "    Args:\n",
    "        model: 模型\n",
    "        src: 源序列（单句，shape: [1, src_seq_len]）\n",
    "        src_mask: 源序列掩码\n",
    "        max_len: 最大生成长度\n",
    "        start_symbol: 开始符号索引\n",
    "        beam_size: 束宽（候选序列数，通常取 5~10，越大质量越高但速度越慢）\n",
    "        length_penalty: 长度惩罚因子（通常取 0.6~1.0，惩罚过短序列，避免语义不完整）\n",
    "        \n",
    "    Returns:\n",
    "        解码后的最优序列（shape: [1, trg_seq_len]）\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    import torch.nn.functional as F  # 导入 F 用于 log_softmax\n",
    "    \n",
    "    # 编码源序列：得到源特征 memory（仅需编码一次，所有候选序列共享）\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    # 初始化束：列表存储（序列 token 索引列表，累积 log 概率），初始为[(<sos>, 0.0)]\n",
    "    beams = [([start_symbol], 0.0)]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        all_candidates = []  # 存储所有扩展后的候选序列\n",
    "        \n",
    "        # 对每个现有候选序列进行扩展\n",
    "        for seq, score in beams:\n",
    "            # 如果序列已以<eos>结束，不再扩展（避免生成冗余 token）\n",
    "            if seq[-1] == TRG.vocab.stoi['<eos>']:\n",
    "                all_candidates.append((seq, score))\n",
    "                continue\n",
    "                \n",
    "            # 准备当前序列的张量输入（shape: [1, len(seq)]）\n",
    "            ys = torch.tensor(seq).unsqueeze(0).to(device)\n",
    "            \n",
    "            # 创建目标序列掩码（逻辑与贪婪搜索一致）\n",
    "            trg_mask = (ys != TRG.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(2)\n",
    "            nopeak_mask = torch.triu(torch.ones(trg_mask.size(0), trg_mask.size(1), trg_mask.size(2)), \n",
    "                                    diagonal=1).to(device) == 0\n",
    "            trg_mask = trg_mask & nopeak_mask\n",
    "            \n",
    "            # 解码：计算下一个词的概率分布\n",
    "            with torch.no_grad():\n",
    "                out = model.decode(memory, src_mask, ys, trg_mask)  # out shape: [1, len(seq), d_model]\n",
    "                prob = model.generator(out[:, -1])  # shape: [1, trg_vocab_size]\n",
    "                log_prob = F.log_softmax(prob, dim=1)  # 转换为 log 概率，避免数值下溢\n",
    "                \n",
    "            # 获取当前步概率最高的 beam_size 个词（减少计算量，无需扩展所有词）\n",
    "            topk_prob, topk_idx = torch.topk(log_prob, beam_size, dim=1)\n",
    "            \n",
    "            # 生成新的候选序列：原序列 + 新词，累积概率 = 原分数 + 新词 log 概率\n",
    "            for j in range(beam_size):\n",
    "                candidate_seq = seq + [topk_idx[0, j].item()]\n",
    "                candidate_score = score + topk_prob[0, j].item()\n",
    "                all_candidates.append((candidate_seq, candidate_score))\n",
    "                \n",
    "        # 按累积概率降序排序，保留前 beam_size 个候选序列\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[1], reverse=True)\n",
    "        beams = ordered[:beam_size]\n",
    "        \n",
    "        # 提前停止条件：所有候选序列都已生成<eos>（无需继续扩展）\n",
    "        if all(seq[-1] == TRG.vocab.stoi['<eos>'] for seq, _ in beams):\n",
    "            break\n",
    "            \n",
    "    # 应用长度惩罚：纠正\"短序列概率高\"的偏差（如\"我吃饭\"比\"我正在吃饭\"短，概率可能更高但语义不完整）\n",
    "    # 惩罚公式：score = 累积 log 概率 / (序列长度 ^ length_penalty)，长度越长惩罚越小\n",
    "    best_seq = None\n",
    "    best_score = -float('inf')\n",
    "    for seq, score in beams:\n",
    "        # 序列长度需排除<sos>和<eos>吗？此处用原始长度（含边界符号），因生成时已包含完整逻辑\n",
    "        length_penalized_score = score / (len(seq) ** length_penalty)\n",
    "        if length_penalized_score > best_score:\n",
    "            best_score = length_penalized_score\n",
    "            best_seq = seq\n",
    "            \n",
    "    return torch.tensor(best_seq).unsqueeze(0)  # 转换为张量返回"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211ee8be",
   "metadata": {},
   "source": [
    "## 5. 模型评估\n",
    "\n",
    "### 5.1 翻译函数\n",
    "\n",
    "翻译函数是解码算法的上层封装，负责将原始文本（如德语句子）转换为模型可处理的数值序列，调用束搜索解码后，再将数值序列转换为可读的目标语言文本（如英语句子）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0589f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, src_field, trg_field, max_len=50, beam_size=5):\n",
    "    \"\"\"\n",
    "    翻译单个句子\n",
    "    \n",
    "    Args:\n",
    "        sentence: 源语言句子（原始文本，如\"Ich liebe dich.\"）\n",
    "        model: 模型\n",
    "        src_field: 源语言 Field（含分词、词汇表等预处理逻辑）\n",
    "        trg_field: 目标语言 Field\n",
    "        max_len: 最大生成长度\n",
    "        beam_size: 束宽\n",
    "        \n",
    "    Returns:\n",
    "        翻译结果（目标语言文本，如\"I love you.\"）\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. 文本预处理：分词 → 添加边界符号 → 数值化\n",
    "    tokenized = src_field.tokenize(sentence)  # 分词（如\"Ich liebe dich.\" → [\"ich\", \"liebe\", \"dich\", \".\"]）\n",
    "    tokenized = [src_field.init_token] + tokenized + [src_field.eos_token]  # 添加<sos>和<eos>\n",
    "    numericalized = [src_field.vocab.stoi[token] for token in tokenized]  # 数值化（词→索引）\n",
    "    \n",
    "    # 2. 转换为张量并创建掩码\n",
    "    src_tensor = torch.LongTensor(numericalized).unsqueeze(0).to(device)  # shape: [1, src_seq_len]\n",
    "    src_mask = (src_tensor != src_field.vocab.stoi['<pad>']).unsqueeze(1).unsqueeze(2)  # 源掩码\n",
    "    \n",
    "    # 3. 束搜索解码\n",
    "    trg_indexes = beam_search_decode(model, src_tensor, src_mask, max_len, \n",
    "                                    trg_field.vocab.stoi[trg_field.init_token],  # <sos>的索引\n",
    "                                    beam_size)\n",
    "    \n",
    "    # 4. 数值序列→文本：索引→词 → 移除<sos>和<eos> → 拼接为句子\n",
    "    trg_tokens = [trg_field.vocab.itos[i] for i in trg_indexes[0]]  # 索引→词（itos: index to string）\n",
    "    trg_tokens = trg_tokens[1:-1]  # 移除<sos>（第一个词）和<eos>（最后一个词）\n",
    "    \n",
    "    return ' '.join(trg_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d718fea3",
   "metadata": {},
   "source": [
    "### 5.2 BLEU 分数评估\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) 是机器翻译中最常用的自动评估指标，通过比较机器翻译输出（hypothesis）与参考翻译（reference）的**n-gram 重叠度**来评估质量。n-gram 是指连续的 n 个词，如 1-gram（单个词）、2-gram（词组）、3-gram（短语）等，重叠度越高，BLEU 分数越高（满分 100）。\n",
    "\n",
    "BLEU 的核心逻辑：\n",
    "1. 计算各阶 n-gram 的精确率（precision）：预测中出现的 n-gram 在参考中出现的比例；\n",
    "2. 应用** brevity penalty（简短惩罚）**：若预测序列比参考序列短太多，降低分数（避免\"短而准\"的无意义翻译）；\n",
    "3. 对各阶精确率取几何平均，得到最终 BLEU 分数。\n",
    "\n",
    "此处使用 NLTK 库的`corpus_bleu`函数，并通过`SmoothingFunction().method4`处理短序列或零重叠的情况（避免 BLEU 分数为 0，确保评估稳定性）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a427d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(data, model, src_field, trg_field, max_len=50, beam_size=5):\n",
    "    \"\"\"\n",
    "    计算整个数据集的 BLEU 分数\n",
    "    \n",
    "    Args:\n",
    "        data: 测试数据集（含源序列和参考目标序列）\n",
    "        model: 模型\n",
    "        src_field: 源语言 Field\n",
    "        trg_field: 目标语言 Field\n",
    "        max_len: 最大生成长度\n",
    "        beam_size: 束宽\n",
    "        \n",
    "    Returns:\n",
    "        BLEU 分数（0~1，乘以 100 后为百分比）\n",
    "    \"\"\"\n",
    "    trgs = []  # 存储所有参考序列（列表的列表，每个参考序列是词的列表）\n",
    "    pred_trgs = []  # 存储所有预测序列（每个预测序列是词的列表）\n",
    "    \n",
    "    for example in data:\n",
    "        # 获取源序列（原始词列表）和参考目标序列\n",
    "        src = vars(example)['src']  # 源序列（如[\"ich\", \"liebe\", \"dich\", \".\"]）\n",
    "        trg = vars(example)['trg']  # 参考目标序列（如[\"i\", \"love\", \"you\", \".\"]）\n",
    "        \n",
    "        # 处理参考序列：添加<sos>和<eos>，符合模型训练时的目标序列格式\n",
    "        trg = [trg_field.init_token] + trg + [trg_field.eos_token]\n",
    "        trgs.append([trg])  # corpus_bleu 要求参考序列为\"列表的列表\"（支持多参考）\n",
    "        \n",
    "        # 处理预测序列：调用 translate_sentence 得到预测文本，再分词为词列表\n",
    "        pred_trg = translate_sentence(' '.join(src), model, src_field, trg_field, max_len, beam_size)\n",
    "        pred_trgs.append(pred_trg.split())  # 分裂为词列表（如\"I love you.\" → [\"i\", \"love\", \"you\", \".\"]）\n",
    "    \n",
    "    # 计算 BLEU 分数：使用 method4 平滑，避免零分数\n",
    "    smooth = SmoothingFunction().method4\n",
    "    bleu_score = corpus_bleu(trgs, pred_trgs, smoothing_function=smooth)\n",
    "    \n",
    "    return bleu_score\n",
    "\n",
    "# 加载最佳模型：使用训练过程中保存的\"验证损失最低\"的模型参数，确保评估泛化能力\n",
    "model.load_state_dict(torch.load('best-model.pt'))\n",
    "\n",
    "# 计算测试集 BLEU 分数\n",
    "bleu_score = calculate_bleu(test_data, model, SRC, TRG)\n",
    "print(f'BLEU 分数: {bleu_score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c65792",
   "metadata": {},
   "source": [
    "### 5.3 推理翻译\n",
    "\n",
    "通过实际示例验证模型的翻译效果，选择常见的德语句子，观察模型是否能生成语义正确、语法通顺的英语翻译。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ae6a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试一些示例翻译\n",
    "examples = [\n",
    "    \"Ich liebe dich.\",  # 德语：我爱你。\n",
    "    \"Das Wetter ist heute schön.\",  # 德语：今天天气很好。\n",
    "    \"Wie geht es dir?\",  # 德语：你好吗？\n",
    "    \"Könnten Sie mir bitte helfen?\"  # 德语：您能帮我一下吗？\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    translation = translate_sentence(example, model, SRC, TRG)\n",
    "    print(f\"德语: {example}\")\n",
    "    print(f\"英语: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab41f92a",
   "metadata": {},
   "source": [
    "在 10 个 epoch 的训练后，预期的 BLEU 分数大约在 15-25 之间（满分为 100）。这个分数对于小型模型（d_model=256，3 层）和有限的训练时间来说是合理的——若增大模型规模（d_model=512，6 层）、增加训练 epoch（如 30 个）或使用更大的数据集（如 WMT 数据集），BLEU 分数可提升至 30 以上，甚至接近人类翻译水平。\n",
    "\n",
    "示例输出（模型训练充分后）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d5808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "德语: Ich liebe dich.\n",
    "英语: i love you.\n",
    "\n",
    "德语: Das Wetter ist heute schön.\n",
    "英语: the weather is nice today.\n",
    "\n",
    "德语: Wie geht es dir?\n",
    "英语: how are you?\n",
    "\n",
    "德语: Könnten Sie mir bitte helfen?\n",
    "英语: could you please help me?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe186b",
   "metadata": {},
   "source": [
    "## 6. 总结\n",
    "\n",
    "在本实验中，我们完成使用 torchtext 加载 IWSLT 2016 英德数据集，并进行分词和词汇表构建。实现推理解码，最后使用 BEU 进行评分。\n",
    "\n",
    "总体而言，本实验成功地将 Transformer 模型应用于英德翻译任务，并验证了多种优化技术的有效性。通过调整模型架构和训练参数，可以进一步提高翻译质量。不仅将 Transformer 模型应用于真实的机器翻译任务，还学习了工业界常用的优化技术和评估方法。这些技术对于训练高质量的大模型至关重要，也是深度学习实践中不可或缺的部分。\n",
    "\n",
    "你可以尝试调整超参数（如模型大小、学习率调度参数、束搜索宽度等），观察它们对翻译质量的影响，进一步加深对机器翻译任务的理解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
